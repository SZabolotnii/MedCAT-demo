# –ü–ª–∞–Ω –ï–≤–æ–ª—é—Ü—ñ—ó –°–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –®–∞—Ä—É: SpaCy ‚Üí Bio_ClinicalBERT

## –û—Å–Ω–æ–≤–Ω–∞ –§—ñ–ª–æ—Å–æ—Ñ—ñ—è –î–∏–∑–∞–π–Ω—É

**"–†–æ–∑—É–º–Ω–µ —Å–ø—Ä–æ—â–µ–Ω–Ω—è —Å—Ç–≤–æ—Ä—é—î –º—ñ—Ü–Ω—ñ —Å–∏—Å—Ç–µ–º–∏"**

### –ö–ª—é—á–æ–≤—ñ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ü—Ä–∏–Ω—Ü–∏–ø–∏

**–ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –ï–≤–æ–ª—é—Ü—ñ—è**: –ü–æ—á–∏–Ω–∞—Ç–∏ –∑ –ø—Ä–æ—Å—Ç–∏—Ö, –Ω–∞–¥—ñ–π–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ —Ç–∞ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É–≤–∞—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –µ–º–ø—ñ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø—Ä–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å.

**–ú–æ–¥—É–ª—å–Ω–∞ –Ü–∑–æ–ª—è—Ü—ñ—è**: –ö–æ–∂–µ–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –∑–∞–º—ñ–Ω–Ω–∏–º –±–µ–∑ –≤–ø–ª–∏–≤—É –Ω–∞ core business logic.

**Human-Centric Implementation**: –ú—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–µ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞ –∫–æ–º–∞–Ω–¥—É —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤ —á–µ—Ä–µ–∑ –ø–æ—Å—Ç—É–ø–æ–≤–µ –≤–≤–µ–¥–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ.

**Data-Driven Decisions**: –ï–≤–æ–ª—é—Ü—ñ—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –∞ –Ω–µ –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏—Ö —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—è—Ö.

---

## –°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–∞ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –°–∏—Å—Ç–µ–º–∏

### –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–∞ –ú–æ–¥–µ–ª—å: –¢—Ä–∏ –†—ñ–≤–Ω—ñ –ê–±—Å—Ç—Ä–∞–∫—Ü—ñ—ó

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –†—ñ–≤–µ–Ω—å 1: Business Logic (CustomCAT Integration Layer)     ‚îÇ
‚îÇ ‚Ä¢ Entity Detection Orchestration                           ‚îÇ
‚îÇ ‚Ä¢ Value Hint Resolution                                     ‚îÇ
‚îÇ ‚Ä¢ Combined Hints Processing                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –†—ñ–≤–µ–Ω—å 2: Semantic Abstraction (VectorizationEngine)       ‚îÇ
‚îÇ ‚Ä¢ Unified Interface –¥–ª—è –≤—Å—ñ—Ö –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ñ–≤                ‚îÇ
‚îÇ ‚Ä¢ Performance Monitoring                                    ‚îÇ
‚îÇ ‚Ä¢ Automatic Evolution Triggers                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –†—ñ–≤–µ–Ω—å 3: Implementation Layer (Swappable Vectorizers)     ‚îÇ
‚îÇ ‚Ä¢ SpaCyVectorizer (Phase 1B.1)                            ‚îÇ
‚îÇ ‚Ä¢ Bio_ClinicalBERTVectorizer (Phase 1B.2)                 ‚îÇ
‚îÇ ‚Ä¢ Future: Custom Ensemble Models                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ü–µ—Ä–µ–≤–∞–≥–∏ –ü—ñ–¥—Ö–æ–¥—É

**–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∞ –ù–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å**: Business logic –Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ–π–Ω–æ—ó —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó

**–ü–æ—Å—Ç—É–ø–æ–≤–∞ –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å**: –ö–æ–º–∞–Ω–¥–∞ –æ—Å–≤–æ—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ç–∏ –∑ –∑–Ω–∞–π–æ–º–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤

**–í–∏–º—ñ—Ä—é–≤–∞–Ω–∞ –ï–≤–æ–ª—é—Ü—ñ—è**: –ö–æ–∂–µ–Ω –∫—Ä–æ–∫ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏–π –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

**–§—ñ–Ω–∞–Ω—Å–æ–≤–∞ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è**: –Ü–Ω–≤–µ—Å—Ç–∏—Ü—ñ—ó –≤ GPU —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–∏–ø—Ä–∞–≤–¥–∞–Ω—ñ –µ–º–ø—ñ—Ä–∏—á–Ω–∏–º–∏ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏

---

## –î–µ—Ç–∞–ª—å–Ω–∏–π –ü–ª–∞–Ω –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—ó

### Phase 1B.1: SpaCy Vectorization Foundation

#### –¢–∏–∂–¥–µ–Ω—å 1: –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ –û—Å–Ω–æ–≤–∞

**–î–µ–Ω—å 1-2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ê–±—Å—Ç—Ä–∞–∫—Ü—ñ–π–Ω–æ–≥–æ –®–∞—Ä—É**

```python
# src/vectorization/core/engine_interface.py
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional
import numpy as np

class VectorizationEngine(ABC):
    """–£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—ó –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó.
    
    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ —Ü—ñ–Ω–Ω—ñ—Å—Ç—å: –î–æ–∑–≤–æ–ª—è—î seamless –∑–∞–º—ñ–Ω—É –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ñ–≤
    –±–µ–∑ –∑–º—ñ–Ω —É business logic.
    """
    
    @abstractmethod
    def embed_text(self, text: str) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è embedding –¥–ª—è –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É."""
        pass
    
    @abstractmethod
    def build_concept_index(self, concepts: Dict[str, str]) -> None:
        """–ü–æ–±—É–¥–æ–≤–∞ —ñ–Ω–¥–µ–∫—Å—É –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É."""
        pass
    
    @abstractmethod
    def find_similar(
        self, 
        query: str, 
        top_k: int = 5,
        min_similarity: float = 0.6
    ) -> List[Tuple[str, float]]:
        """–ü–æ—à—É–∫ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤."""
        pass
    
    @abstractmethod
    def get_performance_metrics(self) -> Dict[str, float]:
        """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –µ–≤–æ–ª—é—Ü—ñ—ó."""
        pass

# src/vectorization/core/performance_monitor.py
class VectorizationPerformanceMonitor:
    """–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –¥–ª—è data-driven –µ–≤–æ–ª—é—Ü—ñ–π–Ω–∏—Ö —Ä—ñ—à–µ–Ω—å."""
    
    def __init__(self):
        self.metrics_history: List[Dict[str, Any]] = []
        self.evolution_triggers = {
            'recall_threshold': 0.78,
            'precision_threshold': 0.80,
            'processing_time_limit_ms': 100,
            'business_satisfaction_score': 3.5  # –∑ 5
        }
    
    def record_session_metrics(
        self,
        documents_processed: int,
        recall: float,
        precision: float,
        f1_score: float,
        avg_processing_time_ms: float,
        semantic_matches_count: int,
        business_feedback_score: Optional[float] = None
    ) -> None:
        """–ó–∞–ø–∏—Å –º–µ—Ç—Ä–∏–∫ —Å–µ—Å—ñ—ó –¥–ª—è —Ç—Ä–µ–Ω–¥-–∞–Ω–∞–ª—ñ–∑—É."""
        
        session_data = {
            'timestamp': datetime.now(),
            'documents_processed': documents_processed,
            'recall': recall,
            'precision': precision,
            'f1_score': f1_score,
            'avg_processing_time_ms': avg_processing_time_ms,
            'semantic_matches_count': semantic_matches_count,
            'business_feedback_score': business_feedback_score
        }
        
        self.metrics_history.append(session_data)
        self._check_evolution_triggers()
    
    def should_evolve_to_bert(self) -> Tuple[bool, str, Dict[str, float]]:
        """–ê–Ω–∞–ª—ñ–∑ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –µ–≤–æ–ª—é—Ü—ñ—ó –¥–æ BERT."""
        if len(self.metrics_history) < 50:  # –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö
            return False, "Insufficient data for evolution decision", {}
        
        recent_sessions = self.metrics_history[-50:]  # –û—Å—Ç–∞–Ω–Ω—ñ 50 —Å–µ—Å—ñ–π
        avg_metrics = self._calculate_average_metrics(recent_sessions)
        
        # –ê–Ω–∞–ª—ñ–∑ —Ç—Ä–∏–≥–µ—Ä—ñ–≤ –µ–≤–æ–ª—é—Ü—ñ—ó
        evolution_reasons = []
        
        if avg_metrics['recall'] < self.evolution_triggers['recall_threshold']:
            evolution_reasons.append(f"Recall {avg_metrics['recall']:.3f} < {self.evolution_triggers['recall_threshold']}")
        
        if avg_metrics['precision'] < self.evolution_triggers['precision_threshold']:
            evolution_reasons.append(f"Precision {avg_metrics['precision']:.3f} < {self.evolution_triggers['precision_threshold']}")
        
        # –ë—ñ–∑–Ω–µ—Å-–∫—Ä–∏—Ç–µ—Ä—ñ–π: –∑–≤–æ—Ä–æ—Ç–Ω–∏–π –∑–≤'—è–∑–æ–∫ –º–µ–¥–∏—á–Ω–∏—Ö –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤
        if avg_metrics.get('business_feedback_score', 5.0) < self.evolution_triggers['business_satisfaction_score']:
            evolution_reasons.append(f"Business satisfaction {avg_metrics.get('business_feedback_score'):.1f} < {self.evolution_triggers['business_satisfaction_score']}")
        
        should_evolve = len(evolution_reasons) > 0
        reason = "; ".join(evolution_reasons) if should_evolve else "Performance meets requirements"
        
        return should_evolve, reason, avg_metrics
```

**–î–µ–Ω—å 3-4: SpaCy Vectorizer Implementation**

```python
# src/vectorization/implementations/spacy_vectorizer.py
import spacy
import numpy as np
import faiss
from typing import Dict, List, Tuple, Optional
from ..core.engine_interface import VectorizationEngine

class SpaCyVectorizer(VectorizationEngine):
    """Production-ready SpaCy –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
    
    def __init__(self, model_name: str = "en_core_web_md"):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –≤–µ–∫—Ç–æ—Ä—ñ–≤."""
        try:
            self.nlp = spacy.load(model_name)
        except OSError:
            raise RuntimeError(f"SpaCy –º–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: python -m spacy download {model_name}")
        
        if not self.nlp.vocab.vectors.size:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –º—ñ—Å—Ç–∏—Ç—å word vectors")
        
        # –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        self.concept_embeddings: Optional[np.ndarray] = None
        self.concept_index: Optional[faiss.Index] = None
        self.cui_list: List[str] = []
        self.performance_metrics: Dict[str, float] = {}
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        self.vector_dimension = self.nlp.vocab.vectors.shape[1]
        self.stopwords = self.nlp.Defaults.stop_words
        
        print(f"‚úÖ SpaCy –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {model_name} ({self.vector_dimension}D vectors)")
    
    def embed_text(self, text: str) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è document embedding —á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è token vectors."""
        doc = self.nlp(text)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–Ω–∞—á—É—â–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤
        meaningful_tokens = [
            token for token in doc 
            if (not token.is_stop and 
                not token.is_punct and 
                not token.is_space and 
                token.has_vector and
                len(token.text) > 2)
        ]
        
        if not meaningful_tokens:
            # Fallback: –≤—Å—ñ —Ç–æ–∫–µ–Ω–∏ –∑ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
            meaningful_tokens = [token for token in doc if token.has_vector]
        
        if not meaningful_tokens:
            # –û—Å—Ç–∞–Ω–Ω—ñ–π fallback: –Ω—É–ª—å–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä
            return np.zeros(self.vector_dimension)
        
        # Weighted averaging: –±—ñ–ª—å—à–∞ –≤–∞–≥–∞ –¥–ª—è —ñ–º–µ–Ω–Ω–∏–∫—ñ–≤ —Ç–∞ –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫—ñ–≤
        vectors = []
        weights = []
        
        for token in meaningful_tokens:
            vectors.append(token.vector)
            # –í–∏—â–∞ –≤–∞–≥–∞ –¥–ª—è –º–µ–¥–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω –º–æ–≤–∏
            weight = 1.5 if token.pos_ in ['NOUN', 'ADJ'] else 1.0
            weights.append(weight)
        
        vectors = np.array(vectors)
        weights = np.array(weights)
        
        # –ó–≤–∞–∂–µ–Ω–µ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è
        weighted_average = np.average(vectors, axis=0, weights=weights)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è cosine similarity
        norm = np.linalg.norm(weighted_average)
        if norm > 0:
            return weighted_average / norm
        else:
            return weighted_average
    
    def build_concept_index(self, concepts: Dict[str, str]) -> None:
        """–ü–æ–±—É–¥–æ–≤–∞ FAISS —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É —Å—Ö–æ–∂–æ—Å—Ç—ñ."""
        print(f"üîß –ü–æ–±—É–¥–æ–≤–∞ SpaCy concept index –¥–ª—è {len(concepts)} –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤...")
        
        start_time = time.time()
        concept_embeddings = []
        self.cui_list = []
        
        # Batch processing –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        concept_items = list(concepts.items())
        batch_size = 100
        
        for i in range(0, len(concept_items), batch_size):
            batch = concept_items[i:i + batch_size]
            
            for cui, preferred_name in batch:
                embedding = self.embed_text(preferred_name)
                concept_embeddings.append(embedding)
                self.cui_list.append(cui)
        
        self.concept_embeddings = np.array(concept_embeddings, dtype=np.float32)
        
        # FAISS index –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
        self.concept_index = faiss.IndexFlatIP(self.vector_dimension)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è cosine similarity
        faiss.normalize_L2(self.concept_embeddings)
        self.concept_index.add(self.concept_embeddings)
        
        build_time = time.time() - start_time
        self.performance_metrics['index_build_time_seconds'] = build_time
        self.performance_metrics['concepts_indexed'] = len(self.cui_list)
        
        print(f"‚úÖ SpaCy —ñ–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤–∏–π: {len(self.cui_list)} –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤ –∑–∞ {build_time:.2f}—Å")
    
    def find_similar(
        self, 
        query: str, 
        top_k: int = 5,
        min_similarity: float = 0.6
    ) -> List[Tuple[str, float]]:
        """–ü–æ—à—É–∫ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤."""
        if self.concept_index is None:
            raise RuntimeError("Concept index –Ω–µ –ø–æ–±—É–¥–æ–≤–∞–Ω–æ. –í–∏–∫–ª–∏—á—Ç–µ build_concept_index() —Å–ø–æ—á–∞—Ç–∫—É.")
        
        start_time = time.time()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è query embedding
        query_embedding = self.embed_text(query).astype(np.float32).reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        # FAISS –ø–æ—à—É–∫
        similarities, indices = self.concept_index.search(query_embedding, top_k)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º –ø–æ—Ä–æ–≥–æ–º
        results = []
        for idx, similarity in zip(indices[0], similarities[0]):
            if similarity >= min_similarity:
                cui = self.cui_list[idx]
                results.append((cui, float(similarity)))
        
        query_time = (time.time() - start_time) * 1000  # ms
        self.performance_metrics['last_query_time_ms'] = query_time
        
        return results
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É."""
        return {
            **self.performance_metrics,
            'vector_dimension': float(self.vector_dimension),
            'memory_usage_mb': self._estimate_memory_usage(),
            'infrastructure_type': 'cpu_only'
        }
    
    def _estimate_memory_usage(self) -> float:
        """–û—Ü—ñ–Ω–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ –≤ MB."""
        if self.concept_embeddings is not None:
            embeddings_size = self.concept_embeddings.nbytes / (1024 * 1024)
            index_size = embeddings_size * 1.5  # FAISS overhead
            return embeddings_size + index_size
        return 0.0
```

**–î–µ–Ω—å 5: –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ CustomCAT**

```python
# src/custom_cat_v2.py (—Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è)
class CustomCAT:
    """–†–æ–∑—à–∏—Ä–µ–Ω–∏–π CustomCAT –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º —à–∞—Ä–æ–º Phase 1B.1"""
    
    def __init__(self, 
                 model_pack_path: str | Path,
                 *,
                 combined_hints_path: str | Path | None = None,
                 enable_semantic: bool = False,
                 semantic_config: Dict[str, Any] = None):
        
        # –Ü—Å–Ω—É—é—á–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Phase 1A
        # ... (existing code) ...
        
        # Phase 1B.1: –°–µ–º–∞–Ω—Ç–∏—á–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        self.semantic_enabled = enable_semantic
        if enable_semantic:
            self._initialize_semantic_layer(semantic_config or {})
    
    def _initialize_semantic_layer(self, config: Dict[str, Any]) -> None:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ —à–∞—Ä—É –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é."""
        from .vectorization.implementations.spacy_vectorizer import SpaCyVectorizer
        from .vectorization.core.performance_monitor import VectorizationPerformanceMonitor
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        vectorizer_type = config.get('vectorizer_type', 'spacy')
        if vectorizer_type == 'spacy':
            spacy_config = config.get('spacy_config', {})
            model_name = spacy_config.get('model', 'en_core_web_md')
            self.vectorizer = SpaCyVectorizer(model_name)
        else:
            raise ValueError(f"Unsupported vectorizer type: {vectorizer_type}")
        
        # –ü–æ–±—É–¥–æ–≤–∞ —ñ–Ω–¥–µ–∫—Å—É –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤
        concepts = self._extract_concepts_from_cdb()
        self.vectorizer.build_concept_index(concepts)
        
        # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        self.performance_monitor = VectorizationPerformanceMonitor()
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É
        self.semantic_config = {
            'similarity_threshold': config.get('similarity_threshold', 0.65),
            'max_candidates_per_document': config.get('max_candidates', 20),
            'enable_noun_chunks': config.get('enable_noun_chunks', True),
            'enable_performance_monitoring': config.get('enable_monitoring', True)
        }
        
        print(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π —à–∞—Ä –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ: {vectorizer_type}")
    
    def extract_entities(self, text: str, *, min_confidence: float = 0.0) -> Dict[str, Any]:
        """–†–æ–∑—à–∏—Ä–µ–Ω–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º Enhancement."""
        
        start_time = time.time()
        
        # Phase 1A: Dictionary-based extraction (—ñ—Å–Ω—É—é—á–∏–π –∫–æ–¥)
        result = self.cat.get_entities(text, only_cui=False)
        entities = result.setdefault("entities", {})
        
        # Combined hints processing (—ñ—Å–Ω—É—é—á–∏–π –∫–æ–¥)
        matches = self.combined_matcher.find_matches(text)
        if matches:
            result.setdefault("combined_hint_matches", matches)
            # ... (existing combined hints logic) ...
        
        # Phase 1B.1: –°–µ–º–∞–Ω—Ç–∏—á–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
        semantic_entities_added = 0
        if self.semantic_enabled:
            semantic_entities = self._find_semantic_entities(text, entities)
            
            for semantic_entity in semantic_entities:
                entity_key = self._next_entity_key(entities)
                semantic_entity['semantic_source'] = True  # –ú—ñ—Ç–∫–∞ –¥–ª—è attribution
                entities[entity_key] = semantic_entity
                semantic_entities_added += 1
        
        # Value rules –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è (—ñ—Å–Ω—É—é—á–∏–π –∫–æ–¥, –ø—Ä–∞—Ü—é—î –¥–ª—è –≤—Å—ñ—Ö entities)
        self._apply_value_rules(text, entities)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ confidence
        if min_confidence > 0:
            entities = {
                key: ent for key, ent in entities.items()
                if float(ent.get('acc', 0.0)) >= min_confidence
            }
        
        # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        processing_time = (time.time() - start_time) * 1000  # ms
        if self.semantic_enabled and hasattr(self, 'performance_monitor'):
            self._record_processing_metrics(
                processing_time, len(entities), semantic_entities_added
            )
        
        result['entities'] = entities
        result['processing_time_ms'] = processing_time
        result['semantic_entities_added'] = semantic_entities_added
        
        return result
    
    def _find_semantic_entities(
        self, 
        text: str, 
        existing_entities: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Phase 1B.1 —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ gap-filling."""
        
        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è candidate spans –≤—ñ–¥ spaCy pipeline
        doc = self.cat.nlp(text)
        candidates = []
        
        # Named Entity candidates –≤—ñ–¥ spaCy NER
        for ent in doc.ents:
            if ent.label_ in ['PERSON', 'ORG', 'GPE']:  # Skip non-medical entities
                continue
            candidates.append(ent)
        
        # Noun phrase candidates –¥–ª—è missed medical terms
        if self.semantic_config.get('enable_noun_chunks', True):
            for chunk in doc.noun_chunks:
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –º–µ–¥–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ñ—Ä–∞–∑
                if 2 <= len(chunk.text.split()) <= 4:
                    candidates.append(chunk)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è candidates —â–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è –∑ existing entities
        existing_spans = {
            (ent['start'], ent['end']) 
            for ent in existing_entities.values()
            if 'start' in ent and 'end' in ent
        }
        
        filtered_candidates = []
        for candidate in candidates:
            candidate_span = (candidate.start_char, candidate.end_char)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä–µ—Ç–∏–Ω—É –∑ existing entities
            overlaps = any(
                self._spans_overlap(candidate_span, existing_span)
                for existing_span in existing_spans
            )
            
            if not overlaps and len(candidate.text.strip()) >= 3:
                filtered_candidates.append(candidate)
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ candidates –¥–ª—è performance
        max_candidates = self.semantic_config.get('max_candidates_per_document', 20)
        filtered_candidates = filtered_candidates[:max_candidates]
        
        # –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –¥–ª—è filtered candidates
        semantic_entities = []
        excluded_cuis = {
            str(ent.get('cui')).upper() for ent in existing_entities.values()
            if ent.get('cui')
        }
        
        for candidate in filtered_candidates:
            candidate_text = candidate.text.strip()
            
            # –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫
            similar_concepts = self.vectorizer.find_similar(
                candidate_text,
                top_k=3,
                min_similarity=self.semantic_config.get('similarity_threshold', 0.65)
            )
            
            for cui, similarity in similar_concepts:
                cui_upper = str(cui).upper()
                if cui_upper in excluded_cuis:
                    continue
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è semantic entity
                entity = self._create_semantic_entity(
                    candidate, cui_upper, similarity
                )
                semantic_entities.append(entity)
                excluded_cuis.add(cui_upper)  # –£–Ω–∏–∫–Ω–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
                break  # –û–¥–∏–Ω match –Ω–∞ candidate
        
        return semantic_entities
    
    def _create_semantic_entity(
        self, 
        span: Any, 
        cui: str, 
        similarity: float
    ) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è semantic entity —É —Ñ–æ—Ä–º–∞—Ç—ñ CustomCAT."""
        cui_info = self.cdb.cui2info.get(cui, {})
        
        return {
            'cui': cui,
            'start': span.start_char,
            'end': span.end_char,
            'detected_name': span.text.replace(' ', '~'),
            'source_value': span.text,
            'pretty_name': cui_info.get('preferred_name', span.text),
            'acc': float(similarity),  # Semantic similarity —è–∫ confidence
            'context_similarity': float(similarity),
            'type_ids': cui_info.get('type_ids', []),
            'meta_anns': {},
            'semantic_similarity': float(similarity),
            'semantic_source': True
        }
```

#### –¢–∏–∂–¥–µ–Ω—å 2: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –í–∞–ª—ñ–¥–∞—Ü—ñ—è

**–î–µ–Ω—å 1-2: Unit Testing**

```python
# tests/test_spacy_vectorizer.py
import pytest
import numpy as np
from src.vectorization.implementations.spacy_vectorizer import SpaCyVectorizer

class TestSpaCyVectorizer:
    """Comprehensive testing –¥–ª—è SpaCy –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞."""
    
    @pytest.fixture
    def vectorizer(self):
        return SpaCyVectorizer("en_core_web_md")
    
    @pytest.fixture
    def sample_concepts(self):
        return {
            'C001': 'diabetes mellitus',
            'C002': 'hypertension',
            'C003': 'myocardial infarction',
            'C004': 'pneumonia',
            'C005': 'chronic kidney disease'
        }
    
    def test_vectorizer_initialization(self, vectorizer):
        """–¢–µ—Å—Ç —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞."""
        assert vectorizer.nlp is not None
        assert vectorizer.vector_dimension > 0
        assert vectorizer.concept_index is None  # –î–æ build_concept_index
    
    def test_text_embedding_generation(self, vectorizer):
        """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó embeddings."""
        text = "patient has diabetes"
        embedding = vectorizer.embed_text(text)
        
        assert isinstance(embedding, np.ndarray)
        assert embedding.shape == (vectorizer.vector_dimension,)
        assert not np.all(embedding == 0)  # –ù–µ –Ω—É–ª—å–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä
    
    def test_concept_index_building(self, vectorizer, sample_concepts):
        """–¢–µ—Å—Ç –ø–æ–±—É–¥–æ–≤–∏ —ñ–Ω–¥–µ–∫—Å—É –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤."""
        vectorizer.build_concept_index(sample_concepts)
        
        assert vectorizer.concept_index is not None
        assert len(vectorizer.cui_list) == len(sample_concepts)
        assert vectorizer.concept_embeddings.shape[0] == len(sample_concepts)
    
    def test_semantic_similarity_search(self, vectorizer, sample_concepts):
        """–¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É."""
        vectorizer.build_concept_index(sample_concepts)
        
        # –¢–µ—Å—Ç —Ç–æ—á–Ω–æ–≥–æ match
        results = vectorizer.find_similar("diabetes", top_k=3)
        assert len(results) > 0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ diabetes mellitus –º–∞—î –≤–∏—Å–æ–∫—É —Å—Ö–æ–∂—ñ—Å—Ç—å
        diabetes_results = [r for r in results if 'C001' in r[0]]
        assert len(diabetes_results) > 0
        assert diabetes_results[0][1] > 0.7  # –í–∏—Å–æ–∫–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å
    
    def test_performance_metrics(self, vectorizer, sample_concepts):
        """–¢–µ—Å—Ç –∑–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        vectorizer.build_concept_index(sample_concepts)
        vectorizer.find_similar("test query", top_k=1)
        
        metrics = vectorizer.get_performance_metrics()
        
        assert 'index_build_time_seconds' in metrics
        assert 'last_query_time_ms' in metrics
        assert 'memory_usage_mb' in metrics
        assert metrics['concepts_indexed'] == len(sample_concepts)

# tests/test_semantic_integration.py
class TestSemanticIntegration:
    """–Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ–π–Ω—ñ —Ç–µ—Å—Ç–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ —à–∞—Ä—É."""
    
    def test_semantic_recall_improvement(self, custom_cat_with_semantic):
        """–í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è recall –≤—ñ–¥ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ —à–∞—Ä—É."""
        test_cases = [
            {
                'text': 'Patient diagnosed with type 2 diabetes mellitus',
                'expected_improvement': True,  # –û—á—ñ–∫—É—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ entities
                'min_entities': 2  # diabetes + type 2 diabetes
            },
            {
                'text': 'Experiencing severe cardiac chest pain',
                'expected_improvement': True,
                'min_entities': 1  # cardiac pain –∞–±–æ chest pain
            }
        ]
        
        for case in test_cases:
            # Test –∑ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º —à–∞—Ä–æ–º
            result_with_semantic = custom_cat_with_semantic.extract_entities(case['text'])
            entities_count = len(result_with_semantic['entities'])
            
            assert entities_count >= case['min_entities']
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ semantic entities
            semantic_entities = [
                e for e in result_with_semantic['entities'].values()
                if e.get('semantic_source', False)
            ]
            
            if case['expected_improvement']:
                assert len(semantic_entities) > 0, f"No semantic entities found for: {case['text']}"
    
    def test_value_hints_compatibility(self, custom_cat_with_semantic):
        """–¢–µ—Å—Ç —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ value hints –∑ semantic entities."""
        text = "Heart rate elevated at 120 bpm after exercise"
        result = custom_cat_with_semantic.extract_entities(text)
        
        # –ü–æ–≤–∏–Ω–Ω—ñ –∑–Ω–∞–π—Ç–∏ —è–∫ dictionary —Ç–∞–∫ —ñ semantic matches
        entities = result['entities']
        assert len(entities) > 0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ value hints –ø—Ä–∞—Ü—é—é—Ç—å –¥–ª—è semantic entities
        for entity in entities.values():
            if entity.get('semantic_source') and entity['cui'] in NUMERIC_CONCEPT_CUIS:
                assert 'value_hints' in entity, f"Missing value hints for semantic entity {entity['cui']}"
    
    def test_performance_regression(self, custom_cat_phase1a, custom_cat_with_semantic):
        """–¢–µ—Å—Ç –≤—ñ–¥—Å—É—Ç–Ω–æ—Å—Ç—ñ –∑–Ω–∞—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        test_texts = [
            "Patient presents with diabetes and hypertension",
            "History of myocardial infarction and chronic kidney disease",
            "Diagnosed with pneumonia and prescribed antibiotics"
        ]
        
        # Baseline Phase 1A performance
        start_time = time.time()
        for text in test_texts * 10:  # 30 documents
            custom_cat_phase1a.extract_entities(text)
        phase1a_time = time.time() - start_time
        
        # Phase 1B.1 performance
        start_time = time.time()
        for text in test_texts * 10:
            custom_cat_with_semantic.extract_entities(text)
        phase1b_time = time.time() - start_time
        
        # Performance degradation should be reasonable
        slowdown_factor = phase1b_time / phase1a_time
        assert slowdown_factor < 3.0, f"Semantic layer too slow: {slowdown_factor:.1f}x slowdown"
```

**–î–µ–Ω—å 3-4: Performance Benchmarking**

```python
# scripts/benchmark_semantic_performance.py
class SemanticPerformanceBenchmark:
    """Benchmark framework –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ —à–∞—Ä—É."""
    
    def __init__(self):
        self.baseline_metrics = {}
        self.semantic_metrics = {}
    
    def run_comprehensive_benchmark(
        self,
        custom_cat_baseline: CustomCAT,
        custom_cat_semantic: CustomCAT,
        test_corpus: List[str],
        expected_entities_per_doc: int = 5
    ) -> Dict[str, Any]:
        """–ü–æ–≤–Ω–∏–π benchmark –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è baseline vs semantic."""
        
        print("üî¨ –ó–∞–ø—É—Å–∫ comprehensive semantic benchmark...")
        
        # Baseline Phase 1A performance
        baseline_results = self._benchmark_cat_instance(
            custom_cat_baseline, test_corpus, "Phase 1A Baseline"
        )
        
        # Semantic Phase 1B.1 performance  
        semantic_results = self._benchmark_cat_instance(
            custom_cat_semantic, test_corpus, "Phase 1B.1 Semantic"
        )
        
        # Comparative analysis
        comparison = self._analyze_performance_delta(baseline_results, semantic_results)
        
        # Business impact assessment
        business_impact = self._assess_business_impact(comparison, expected_entities_per_doc)
        
        return {
            'baseline_performance': baseline_results,
            'semantic_performance': semantic_results,
            'performance_comparison': comparison,
            'business_impact': business_impact,
            'recommendation': self._generate_recommendation(business_impact)
        }
    
    def _benchmark_cat_instance(
        self,
        cat_instance: CustomCAT,
        test_corpus: List[str],
        instance_name: str
    ) -> Dict[str, float]:
        """Benchmark single CAT instance."""
        
        print(f"   Benchmarking {instance_name}...")
        
        start_time = time.time()
        total_entities = 0
        total_semantic_entities = 0
        processing_times = []
        
        for text in test_corpus:
            doc_start = time.time()
            result = cat_instance.extract_entities(text)
            doc_time = (time.time() - doc_start) * 1000  # ms
            
            processing_times.append(doc_time)
            total_entities += len(result['entities'])
            total_semantic_entities += result.get('semantic_entities_added', 0)
        
        total_time = time.time() - start_time
        
        return {
            'total_processing_time_seconds': total_time,
            'avg_processing_time_ms': np.mean(processing_times),
            'documents_processed': len(test_corpus),
            'total_entities_found': total_entities,
            'avg_entities_per_doc': total_entities / len(test_corpus),
            'total_semantic_entities': total_semantic_entities,
            'semantic_entities_ratio': total_semantic_entities / max(total_entities, 1),
            'docs_per_second': len(test_corpus) / total_time
        }
    
    def _analyze_performance_delta(
        self,
        baseline: Dict[str, float],
        semantic: Dict[str, float]
    ) -> Dict[str, float]:
        """–ê–Ω–∞–ª—ñ–∑ —Ä—ñ–∑–Ω–∏—Ü—ñ –≤ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        
        return {
            'recall_improvement_ratio': semantic['avg_entities_per_doc'] / max(baseline['avg_entities_per_doc'], 1),
            'processing_time_ratio': semantic['avg_processing_time_ms'] / max(baseline['avg_processing_time_ms'], 1),
            'throughput_ratio': semantic['docs_per_second'] / max(baseline['docs_per_second'], 1),
            'additional_entities_per_doc': semantic['avg_entities_per_doc'] - baseline['avg_entities_per_doc'],
            'semantic_contribution_percent': semantic['semantic_entities_ratio'] * 100
        }
    
    def _assess_business_impact(
        self,
        comparison: Dict[str, float],
        expected_entities_per_doc: int
    ) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ –±—ñ–∑–Ω–µ—Å-–≤–ø–ª–∏–≤—É —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ —à–∞—Ä—É."""
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è coverage
        recall_improvement_percent = (comparison['recall_improvement_ratio'] - 1) * 100
        
        # –û—Ü—ñ–Ω–∫–∞ cost/benefit
        processing_overhead_percent = (comparison['processing_time_ratio'] - 1) * 100
        
        # Business value scoring
        if recall_improvement_percent >= 10 and processing_overhead_percent <= 100:
            business_value = 'high'
        elif recall_improvement_percent >= 5 and processing_overhead_percent <= 200:
            business_value = 'medium'
        else:
            business_value = 'low'
        
        return {
            'recall_improvement_percent': recall_improvement_percent,
            'processing_overhead_percent': processing_overhead_percent,
            'business_value_assessment': business_value,
            'additional_entities_per_doc': comparison['additional_entities_per_doc'],
            'meets_performance_threshold': processing_overhead_percent <= 200,
            'meets_recall_threshold': recall_improvement_percent >= 5
        }
    
    def _generate_recommendation(self, business_impact: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –±—ñ–∑–Ω–µ—Å-–≤–ø–ª–∏–≤—É."""
        
        if business_impact['business_value_assessment'] == 'high':
            return "–†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∑ SpaCy semantic layer. –í–∏—Å–æ–∫–∏–π ROI."
        
        elif business_impact['business_value_assessment'] == 'medium':
            if business_impact['meets_recall_threshold']:
                return "–†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ SpaCy semantic layer –∑ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥–æ–º. –†–æ–∑–≥–ª—è–Ω—É—Ç–∏ BERT –µ–≤–æ–ª—é—Ü—ñ—é —á–µ—Ä–µ–∑ 2-4 —Ç–∏–∂–Ω—ñ."
            else:
                return "–†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –†–æ–∑–≥–ª—è–Ω—É—Ç–∏ BERT –µ–≤–æ–ª—é—Ü—ñ—é –∞–±–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é SpaCy –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤."
        
        else:  # low business value
            return "–†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –ó–∞–ª–∏—à–∏—Ç–∏—Å—è –Ω–∞ Phase 1A. –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π —à–∞—Ä –Ω–µ –≤–∏–ø—Ä–∞–≤–¥–æ–≤—É—î overhead."
```

**–î–µ–Ω—å 5: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è —Ç–∞ Deployment**

```python
# Configuration Management
# config/semantic_config.yaml
semantic_layer:
  # Phase 1B.1 Configuration
  enabled: true
  vectorizer_type: "spacy"
  
  spacy_config:
    model: "en_core_web_md"
    similarity_threshold: 0.65
    max_candidates_per_document: 20
    enable_noun_chunks: true
    weighted_averaging: true
    
  performance_monitoring:
    enabled: true
    metrics_history_size: 1000
    evolution_check_frequency: 100  # documents
    
  evolution_triggers:
    recall_threshold: 0.78
    precision_threshold: 0.80
    business_satisfaction_threshold: 3.5
    max_processing_time_ms: 100

# Deployment –≥–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å
# scripts/deploy_semantic_layer.py
def deploy_semantic_enhancement():
    """Deployment script –¥–ª—è Phase 1B.1."""
    
    print("üöÄ Deploying Semantic Layer Phase 1B.1...")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ requirements
    check_spacy_model_availability()
    validate_configuration()
    run_smoke_tests()
    
    # Performance baseline
    establish_performance_baseline()
    
    # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ setup
    setup_performance_monitoring()
    
    print("‚úÖ Semantic layer deployment complete!")
```

### Phase 1B.2: Conditional BERT Evolution

#### –¢–∏–∂–¥–µ–Ω—å 3-4: Performance Evaluation Period

**–°—Ç—Ä–∞—Ç–µ–≥—ñ—è**: –ó–±—ñ—Ä –µ–º–ø—ñ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ BERT –µ–≤–æ–ª—é—Ü—ñ—é.

**–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ Framework**:

```python
# src/vectorization/core/evolution_decision_engine.py
class EvolutionDecisionEngine:
    """Data-driven —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å –ø—Ä–æ –µ–≤–æ–ª—é—Ü—ñ—é –¥–æ BERT."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metrics_collector = VectorizationPerformanceMonitor()
        self.business_feedback_collector = BusinessFeedbackCollector()
        
    def analyze_evolution_necessity(
        self,
        evaluation_period_days: int = 14
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –µ–≤–æ–ª—é—Ü—ñ—ó –¥–æ BERT –ø—ñ—Å–ª—è evaluation period."""
        
        # –ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ –∑–∞ evaluation period
        recent_metrics = self.metrics_collector.get_recent_metrics(evaluation_period_days)
        business_feedback = self.business_feedback_collector.get_recent_feedback(evaluation_period_days)
        
        # –¢–µ—Ö–Ω—ñ—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        technical_analysis = self._analyze_technical_performance(recent_metrics)
        
        # –ë—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑
        business_analysis = self._analyze_business_impact(business_feedback)
        
        # Cost-benefit –∞–Ω–∞–ª—ñ–∑
        cost_benefit = self._calculate_bert_roi(technical_analysis, business_analysis)
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è
        recommendation = self._generate_evolution_recommendation(
            technical_analysis, business_analysis, cost_benefit
        )
        
        return {
            'evaluation_period_days': evaluation_period_days,
            'technical_analysis': technical_analysis,
            'business_analysis': business_analysis,
            'cost_benefit_analysis': cost_benefit,
            'recommendation': recommendation,
            'confidence_score': self._calculate_recommendation_confidence(technical_analysis, business_analysis)
        }
    
    def _analyze_technical_performance(self, metrics: List[Dict]) -> Dict[str, Any]:
        """–¢–µ—Ö–Ω—ñ—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ SpaCy layer."""
        
        if not metrics:
            return {'status': 'insufficient_data'}
        
        avg_recall = np.mean([m['recall'] for m in metrics if 'recall' in m])
        avg_precision = np.mean([m['precision'] for m in metrics if 'precision' in m])
        avg_f1 = np.mean([m['f1_score'] for m in metrics if 'f1_score' in m])
        avg_processing_time = np.mean([m['avg_processing_time_ms'] for m in metrics])
        
        # Performance gap analysis
        recall_gap = max(0, self.config['target_recall'] - avg_recall)
        precision_gap = max(0, self.config['target_precision'] - avg_precision)
        
        # Trend analysis
        recall_trend = self._calculate_trend([m.get('recall', 0) for m in metrics[-30:]])
        
        return {
            'avg_recall': avg_recall,
            'avg_precision': avg_precision,
            'avg_f1': avg_f1,
            'avg_processing_time_ms': avg_processing_time,
            'recall_gap': recall_gap,
            'precision_gap': precision_gap,
            'recall_trend': recall_trend,
            'meets_technical_thresholds': avg_recall >= self.config['target_recall'] and avg_precision >= self.config['target_precision'],
            'performance_stable': abs(recall_trend) < 0.02  # <2% trend
        }
```

#### –¢–∏–∂–¥–µ–Ω—å 5-6: BERT Implementation (Conditional)

**–í–∏–∫–æ–Ω—É—î—Ç—å—Å—è –¢–Ü–õ–¨–ö–ò —è–∫—â–æ Evolution Decision Engine —Ä–µ–∫–æ–º–µ–Ω–¥—É—î –µ–≤–æ–ª—é—Ü—ñ—é**

```python
# src/vectorization/implementations/bert_vectorizer.py
class BioClinicalBERTVectorizer(VectorizationEngine):
    """Production-ready Bio_ClinicalBERT –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä."""
    
    def __init__(self, 
                 model_name: str = "emilyalsentzer/Bio_ClinicalBERT",
                 device: str = "auto",
                 cache_dir: Optional[str] = None):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º device detection."""
        
        # Device selection
        if device == "auto":
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"üß† –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Bio_ClinicalBERT –Ω–∞ {self.device}...")
        
        # Model loading –∑ error handling
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            self.model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)
            self.model.to(self.device)
            self.model.eval()
        except Exception as e:
            raise RuntimeError(f"Failed to load Bio_ClinicalBERT: {e}")
        
        # Architecture components
        self.concept_embeddings: Optional[np.ndarray] = None
        self.concept_index: Optional[faiss.Index] = None
        self.cui_list: List[str] = []
        self.embedding_cache: Dict[str, np.ndarray] = {}
        
        # Performance optimization
        self.batch_size = 32 if self.device.type == 'cuda' else 8
        self.max_sequence_length = 128
        
        print(f"‚úÖ Bio_ClinicalBERT –≥–æ—Ç–æ–≤–∏–π: batch_size={self.batch_size}")
    
    def embed_text(self, text: str) -> np.ndarray:
        """Generate BERT embedding –¥–ª—è –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É."""
        
        # Cache check
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        # Tokenization
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=self.max_sequence_length,
            padding=True
        ).to(self.device)
        
        # Forward pass
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Use [CLS] token embedding
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        # Normalization
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        # Cache –¥–ª—è repeated queries
        if len(self.embedding_cache) < 1000:  # Limit cache size
            self.embedding_cache[text] = embedding
        
        return embedding
    
    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:
        """Batch embedding generation –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        
        if not texts:
            return []
        
        # Filter cached embeddings
        uncached_texts = []
        uncached_indices = []
        results = [None] * len(texts)
        
        for i, text in enumerate(texts):
            if text in self.embedding_cache:
                results[i] = self.embedding_cache[text]
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        if not uncached_texts:
            return results
        
        # Batch processing
        all_embeddings = []
        for i in range(0, len(uncached_texts), self.batch_size):
            batch_texts = uncached_texts[i:i + self.batch_size]
            
            # Tokenization
            inputs = self.tokenizer(
                batch_texts,
                return_tensors='pt',
                truncation=True,
                max_length=self.max_sequence_length,
                padding=True
            ).to(self.device)
            
            # Forward pass
            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            # Normalization
            for embedding in batch_embeddings:
                norm = np.linalg.norm(embedding)
                if norm > 0:
                    embedding = embedding / norm
                all_embeddings.append(embedding)
        
        # Update results —Ç–∞ cache
        for i, embedding in enumerate(all_embeddings):
            result_index = uncached_indices[i]
            results[result_index] = embedding
            
            # Cache update
            if len(self.embedding_cache) < 1000:
                self.embedding_cache[uncached_texts[i]] = embedding
        
        return results
    
    def build_concept_index(self, concepts: Dict[str, str]) -> None:
        """–ü–æ–±—É–¥–æ–≤–∞ BERT-based concept index."""
        print(f"üß† –ü–æ–±—É–¥–æ–≤–∞ Bio_ClinicalBERT index –¥–ª—è {len(concepts)} –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤...")
        
        start_time = time.time()
        
        # Batch embedding generation
        concept_items = list(concepts.items())
        self.cui_list = [cui for cui, _ in concept_items]
        concept_names = [name for _, name in concept_items]
        
        # Generate embeddings –≤ batches
        concept_embeddings = self.embed_batch(concept_names)
        self.concept_embeddings = np.array(concept_embeddings, dtype=np.float32)
        
        # FAISS index
        dimension = self.concept_embeddings.shape[1]
        self.concept_index = faiss.IndexFlatIP(dimension)
        
        # Embeddings –≤–∂–µ normalized
        self.concept_index.add(self.concept_embeddings)
        
        build_time = time.time() - start_time
        print(f"‚úÖ Bio_ClinicalBERT index –≥–æ—Ç–æ–≤–∏–π: {len(self.cui_list)} –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤ –∑–∞ {build_time:.2f}—Å")
    
    def find_similar(
        self, 
        query: str, 
        top_k: int = 5,
        min_similarity: float = 0.75  # –í–∏—â–∏–π threshold –¥–ª—è BERT
    ) -> List[Tuple[str, float]]:
        """BERT-based —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫."""
        
        if self.concept_index is None:
            raise RuntimeError("BERT concept index –Ω–µ –ø–æ–±—É–¥–æ–≤–∞–Ω–æ")
        
        start_time = time.time()
        
        # Generate query embedding
        query_embedding = self.embed_text(query).astype(np.float32).reshape(1, -1)
        
        # FAISS search
        similarities, indices = self.concept_index.search(query_embedding, top_k)
        
        # Filter —ñ format results
        results = []
        for idx, similarity in zip(indices[0], similarities[0]):
            if similarity >= min_similarity:
                cui = self.cui_list[idx]
                results.append((cui, float(similarity)))
        
        query_time = (time.time() - start_time) * 1000  # ms
        
        return results
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """BERT-specific –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."""
        return {
            'vector_dimension': 768.0,  # Bio_ClinicalBERT dimension
            'device_type': self.device.type,
            'batch_size': float(self.batch_size),
            'cache_size': float(len(self.embedding_cache)),
            'infrastructure_type': 'gpu_optimized' if self.device.type == 'cuda' else 'cpu_compatible',
            'memory_usage_mb': self._estimate_gpu_memory_usage()
        }
    
    def _estimate_gpu_memory_usage(self) -> float:
        """–û—Ü—ñ–Ω–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –ø–∞–º'—è—Ç—ñ."""
        if self.device.type == 'cuda':
            return torch.cuda.memory_allocated(self.device) / (1024 * 1024)
        return 0.0
```

### Phase 1B.3: Adaptive Evolution System

```python
# src/vectorization/adaptive/evolution_orchestrator.py
class AdaptiveVectorizationOrchestrator:
    """Intelligent —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –µ–≤–æ–ª—é—Ü—ñ—î—é –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.current_vectorizer: Optional[VectorizationEngine] = None
        self.performance_monitor = VectorizationPerformanceMonitor()
        self.evolution_engine = EvolutionDecisionEngine(config)
        
        # Evolution state tracking
        self.evolution_history: List[Dict[str, Any]] = []
        self.current_phase = "initialization"
        
    def initialize_adaptive_system(self) -> None:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–±–æ—Ä–æ–º optimal vectorizer."""
        
        print("üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Adaptive Vectorization System...")
        
        # Determine initial vectorizer based on configuration
        initial_vectorizer = self._select_initial_vectorizer()
        
        # Initialize chosen vectorizer
        self._switch_to_vectorizer(initial_vectorizer, reason="initial_setup")
        
        # Setup monitoring
        self._setup_continuous_monitoring()
        
        print(f"‚úÖ Adaptive —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞: {type(self.current_vectorizer).__name__}")
    
    def _select_initial_vectorizer(self) -> str:
        """Smart initial vectorizer selection."""
        
        # Check infrastructure capabilities
        gpu_available = torch.cuda.is_available()
        memory_available_gb = self._get_available_memory_gb()
        
        # Check user preferences
        prefer_performance = self.config.get('prefer_performance_over_speed', False)
        budget_constraints = self.config.get('budget_constraints', False)
        
        # Decision logic
        if budget_constraints or not gpu_available:
            return 'spacy'
        
        if prefer_performance and gpu_available and memory_available_gb > 4:
            return 'bio_clinical_bert'
        
        # Default: start conservative
        return 'spacy'
    
    def process_with_adaptive_vectorization(
        self,
        text: str,
        existing_entities: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Process text –∑ adaptive vectorizer —Ç–∞ monitoring."""
        
        start_time = time.time()
        
        # Process with current vectorizer
        semantic_entities = self._process_with_current_vectorizer(text, existing_entities)
        
        processing_time = (time.time() - start_time) * 1000  # ms
        
        # Record performance
        self.performance_monitor.record_session_metrics(
            documents_processed=1,
            semantic_matches_count=len(semantic_entities),
            avg_processing_time_ms=processing_time,
            # Additional metrics would be calculated based on entity analysis
        )
        
        # Check evolution triggers
        self._check_and_execute_evolution()
        
        # Prepare performance metadata
        performance_metadata = {
            'processing_time_ms': processing_time,
            'vectorizer_type': type(self.current_vectorizer).__name__,
            'semantic_entities_count': len(semantic_entities),
            'current_phase': self.current_phase
        }
        
        return semantic_entities, performance_metadata
    
    def _check_and_execute_evolution(self) -> None:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è evolution triggers."""
        
        # Check if enough data accumulated
        if len(self.performance_monitor.metrics_history) % 100 != 0:
            return  # Check every 100 documents
        
        # Run evolution analysis
        should_evolve, reason, current_metrics = self.performance_monitor.should_evolve_to_bert()
        
        if should_evolve and self.current_phase == "spacy_evaluation":
            print(f"üöÄ Evolution trigger activated: {reason}")
            self._execute_bert_evolution(reason, current_metrics)
        
        elif not should_evolve and self.current_phase == "spacy_evaluation":
            print(f"‚úÖ SpaCy performance sufficient: {reason}")
            self.current_phase = "spacy_stable"
    
    def _execute_bert_evolution(self, reason: str, current_metrics: Dict[str, float]) -> None:
        """–í–∏–∫–æ–Ω–∞–Ω–Ω—è –µ–≤–æ–ª—é—Ü—ñ—ó –¥–æ BERT –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞."""
        
        print("üß† Executing evolution to Bio_ClinicalBERT...")
        
        # Record evolution decision
        evolution_record = {
            'timestamp': datetime.now(),
            'from_vectorizer': type(self.current_vectorizer).__name__,
            'to_vectorizer': 'BioClinicalBERTVectorizer',
            'trigger_reason': reason,
            'spacy_baseline_metrics': current_metrics.copy()
        }
        
        try:
            # Initialize BERT vectorizer
            self._switch_to_vectorizer('bio_clinical_bert', reason=reason)
            
            # Reset performance monitoring –¥–ª—è –Ω–æ–≤–æ–≥–æ baseline
            self.performance_monitor = VectorizationPerformanceMonitor()
            
            self.current_phase = "bert_evaluation"
            evolution_record['status'] = 'success'
            
            print("‚úÖ Evolution to Bio_ClinicalBERT completed successfully")
            
        except Exception as e:
            print(f"‚ùå Evolution failed: {e}")
            print("üîÑ Falling back to SpaCy vectorizer")
            
            # Fallback to SpaCy
            self._switch_to_vectorizer('spacy', reason=f"bert_evolution_failed: {e}")
            evolution_record['status'] = 'failed'
            evolution_record['error'] = str(e)
        
        self.evolution_history.append(evolution_record)
    
    def _switch_to_vectorizer(self, vectorizer_type: str, reason: str) -> None:
        """Safe switch –º—ñ–∂ vectorizers."""
        
        print(f"üîÑ Switching to {vectorizer_type} vectorizer (reason: {reason})")
        
        # Create new vectorizer instance
        if vectorizer_type == 'spacy':
            from ..implementations.spacy_vectorizer import SpaCyVectorizer
            new_vectorizer = SpaCyVectorizer(
                model_name=self.config.get('spacy_model', 'en_core_web_md')
            )
        
        elif vectorizer_type == 'bio_clinical_bert':
            from ..implementations.bert_vectorizer import BioClinicalBERTVectorizer
            new_vectorizer = BioClinicalBERTVectorizer(
                device=self.config.get('bert_device', 'auto'),
                cache_dir=self.config.get('bert_cache_dir')
            )
        
        else:
            raise ValueError(f"Unknown vectorizer type: {vectorizer_type}")
        
        # Build concept index
        if hasattr(self, '_concepts_cache'):
            new_vectorizer.build_concept_index(self._concepts_cache)
        
        # Atomic switch
        old_vectorizer = self.current_vectorizer
        self.current_vectorizer = new_vectorizer
        
        # Cleanup old vectorizer
        if old_vectorizer and hasattr(old_vectorizer, 'cleanup'):
            old_vectorizer.cleanup()
    
    def get_system_status(self) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π status adaptive —Å–∏—Å—Ç–µ–º–∏."""
        
        current_metrics = self.current_vectorizer.get_performance_metrics() if self.current_vectorizer else {}
        
        return {
            'current_phase': self.current_phase,
            'current_vectorizer': type(self.current_vectorizer).__name__ if self.current_vectorizer else None,
            'evolution_history_count': len(self.evolution_history),
            'performance_sessions_recorded': len(self.performance_monitor.metrics_history),
            'current_performance_metrics': current_metrics,
            'system_recommendations': self._generate_system_recommendations()
        }
    
    def _generate_system_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π."""
        recommendations = []
        
        if self.current_phase == "spacy_evaluation":
            if len(self.performance_monitor.metrics_history) < 50:
                recommendations.append("–ó–±—ñ—Ä –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è evolution decision")
            else:
                recommendations.append("–î–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è evolution analysis")
        
        elif self.current_phase == "bert_evaluation":
            recommendations.append("–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ BERT performance vs SpaCy baseline")
        
        elif self.current_phase == "spacy_stable":
            recommendations.append("SpaCy performance —Å—Ç–∞–±—ñ–ª—å–Ω–∞, continue monitoring")
        
        # Performance-based recommendations
        if self.current_vectorizer:
            current_metrics = self.current_vectorizer.get_performance_metrics()
            if current_metrics.get('memory_usage_mb', 0) > 1000:
                recommendations.append("–í–∏—Å–æ–∫–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ - consider optimization")
        
        return recommendations
```

---

## –°—Ç—Ä–∞—Ç–µ–≥—ñ—è –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –†–∏–∑–∏–∫–∞–º–∏

### –¢–µ—Ö–Ω—ñ—á–Ω—ñ –†–∏–∑–∏–∫–∏ —Ç–∞ –ú—ñ—Ç–∏–≥–∞—Ü—ñ—è

#### –†–∏–∑–∏–∫ 1: SpaCy Performance –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—è
**–°—Ü–µ–Ω–∞—Ä—ñ–π**: SpaCy vectors –Ω–µ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–≥–æ recall improvement
**–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å**: –°–µ—Ä–µ–¥–Ω—è (30%)
**–ú—ñ—Ç–∏–≥–∞—Ü—ñ—è**:
```python
# Adaptive threshold optimization
class SpaCyOptimizer:
    def optimize_similarity_threshold(self, validation_set):
        """Dynamic threshold optimization based on validation data."""
        best_threshold = 0.6
        best_f1 = 0.0
        
        for threshold in np.arange(0.5, 0.8, 0.05):
            f1_score = self.validate_with_threshold(validation_set, threshold)
            if f1_score > best_f1:
                best_f1 = f1_score
                best_threshold = threshold
        
        return best_threshold
```

#### –†–∏–∑–∏–∫ 2: Infrastructure Compatibility
**–°—Ü–µ–Ω–∞—Ä—ñ–π**: GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–µ –¥–ª—è BERT evolution
**–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å**: –í–∏—Å–æ–∫–∞ (60% environments)
**–ú—ñ—Ç–∏–≥–∞—Ü—ñ—è**: Multi-tier fallback strategy
```python
class InfrastructureAdapter:
    def __init__(self):
        self.capabilities = self._assess_infrastructure()
    
    def get_optimal_vectorizer(self):
        if self.capabilities['gpu_available'] and self.capabilities['memory_gb'] > 4:
            return 'bio_clinical_bert'
        elif self.capabilities['memory_gb'] > 2:
            return 'spacy'
        else:
            return 'lightweight_spacy'  # Smaller model fallback
```

#### –†–∏–∑–∏–∫ 3: Model Performance Regression
**–°—Ü–µ–Ω–∞—Ä—ñ–π**: Semantic layer –ø–æ–≥—ñ—Ä—à—É—î existing dictionary performance
**–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å**: –ù–∏–∑—å–∫–∞ (15%)
**–ú—ñ—Ç–∏–≥–∞—Ü—ñ—è**: Comprehensive regression testing
```python
class RegressionProtection:
    def validate_no_regression(self, baseline_results, enhanced_results):
        """Ensure semantic enhancement doesn't hurt existing performance."""
        
        # Check dictionary entities preservation
        baseline_entities = self._extract_dictionary_entities(baseline_results)
        enhanced_dict_entities = self._extract_dictionary_entities(enhanced_results)
        
        regression_detected = len(enhanced_dict_entities) < len(baseline_entities) * 0.95
        
        if regression_detected:
            raise RegressionError("Dictionary performance degraded by >5%")
```

### –ë—ñ–∑–Ω–µ—Å –†–∏–∑–∏–∫–∏ —Ç–∞ –ú—ñ—Ç–∏–≥–∞—Ü—ñ—è

#### –†–∏–∑–∏–∫ 1: ROI –ù–µ –í–∏–ø—Ä–∞–≤–¥–∞—Ç—å—Å—è
**–°—Ü–µ–Ω–∞—Ä—ñ–π**: Semantic enhancement –Ω–µ –ø—Ä–∏–Ω–æ—Å–∏—Ç—å sufficient business value
**–ú—ñ—Ç–∏–≥–∞—Ü—ñ—è**: Precise ROI measurement framework
```python
class ROICalculator:
    def calculate_semantic_roi(self, baseline_metrics, enhanced_metrics, costs):
        """Calculate ROI based on entity discovery improvement."""
        
        additional_entities_monthly = (
            enhanced_metrics['entities_per_doc'] - baseline_metrics['entities_per_doc']
        ) * self.monthly_document_volume
        
        value_per_entity = self.business_value_per_discovered_entity
        monthly_value_increase = additional_entities_monthly * value_per_entity
        
        monthly_costs = costs['development_amortized'] + costs['infrastructure']
        
        roi_ratio = monthly_value_increase / monthly_costs
        payback_months = costs['initial_investment'] / monthly_value_increase
        
        return {
            'roi_ratio': roi_ratio,
            'payback_months': payback_months,
            'monthly_value_increase': monthly_value_increase,
            'recommendation': 'proceed' if roi_ratio > 1.5 else 'reconsider'
        }
```

---

## Framework –ü—Ä–∏–π–Ω—è—Ç—Ç—è –†—ñ—à–µ–Ω—å

### GO/NO-GO Checkpoints

#### Checkpoint 1: SpaCy Implementation (–ö—ñ–Ω–µ—Ü—å –¢–∏–∂–Ω—è 2)
**–ö—Ä–∏—Ç–µ—Ä—ñ—ó –û—Ü—ñ–Ω–∫–∏**:
- [ ] SpaCy vectorizer —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π –±–µ–∑ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –±–∞–≥—ñ–≤
- [ ] Recall improvement ‚â• 5% –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –∫–æ—Ä–ø—É—Å—ñ
- [ ] Processing time increase ‚â§ 3x baseline
- [ ] Value hints —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞
- [ ] –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º CustomCAT seamless

**–†–µ–∑—É–ª—å—Ç–∞—Ç –†—ñ—à–µ–Ω–Ω—è**:
- ‚úÖ **CONTINUE**: –í—Å—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó –≤–∏–∫–æ–Ω–∞–Ω—ñ ‚Üí –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–∏ –¥–æ evaluation period
- ‚ö†Ô∏è **OPTIMIZE**: –ß–∞—Å—Ç–∫–æ–≤–æ –≤–∏–∫–æ–Ω–∞–Ω—ñ ‚Üí 1 —Ç–∏–∂–¥–µ–Ω—å optimization
- ‚ùå **ROLLBACK**: –ö—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ ‚Üí –ø–æ–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ Phase 1A

#### Checkpoint 2: Evolution Decision (–ö—ñ–Ω–µ—Ü—å –¢–∏–∂–Ω—è 6)
**–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ê–Ω–∞–ª—ñ–∑—É**:
```python
evolution_metrics = {
    'spacy_performance': {
        'avg_recall': 0.82,
        'avg_precision': 0.85,
        'avg_f1': 0.835,
        'avg_processing_time_ms': 45,
        'business_satisfaction': 3.8  # –∑ 5.0
    },
    'business_context': {
        'target_recall': 0.88,
        'acceptable_processing_time_ms': 100,
        'gpu_budget_available': True,
        'team_bandwidth_for_complexity': 'medium'
    }
}
```

**Decision Tree**:
```
SpaCy Performance Analysis
‚îú‚îÄ‚îÄ Recall ‚â• 85% AND F1 ‚â• 83% AND Business Satisfaction ‚â• 4.0
‚îÇ   ‚îî‚îÄ‚îÄ DECISION: SpaCy Sufficient ‚Üí Continue –∑ optimization
‚îú‚îÄ‚îÄ Recall < 78% OR Critical entities missed frequently
‚îÇ   ‚îî‚îÄ‚îÄ DECISION: BERT Evolution Required ‚Üí Proceed to Phase 1B.2
‚îú‚îÄ‚îÄ 78% ‚â§ Recall < 85% AND Processing acceptable
‚îÇ   ‚îú‚îÄ‚îÄ Business feedback positive ‚Üí OPTIMIZE SpaCy parameters
‚îÇ   ‚îú‚îÄ‚îÄ Business demands higher recall ‚Üí BERT Evolution
‚îÇ   ‚îî‚îÄ‚îÄ Budget constraints ‚Üí Continue SpaCy –∑ targeted improvements
‚îî‚îÄ‚îÄ Performance unstable OR processing too slow
    ‚îî‚îÄ‚îÄ DECISION: Technical debt resolution required ‚Üí Optimize architecture
```

#### Checkpoint 3: BERT Evolution Success (–Ø–∫—â–æ –∑–∞—Å—Ç–æ—Å–æ–≤–Ω–æ)
**–ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—ñ –ú–µ—Ç—Ä–∏–∫–∏**:
- [ ] BERT recall improvement ‚â• 5% –Ω–∞–¥ SpaCy
- [ ] Processing time ‚â§ 100ms per document
- [ ] GPU infrastructure stable
- [ ] Memory usage acceptable (‚â§ 2GB)
- [ ] Business satisfaction increase

### –°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—ñ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑–∞ –†–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

#### Scenario A: SpaCy Success (F1 ‚â• 0.83)
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è**: Continue –∑ SpaCy —è–∫ primary vectorizer
**–ù–∞—Å—Ç—É–ø–Ω—ñ –ö—Ä–æ–∫–∏**:
- Optimize SpaCy parameters –¥–ª—è maximum performance  
- Implement continuous monitoring
- Reserve BERT —è–∫ future enhancement option
- Focus on other system improvements

#### Scenario B: BERT Evolution Justified (SpaCy F1 < 0.78)
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è**: Proceed –∑ BERT implementation
**–ù–∞—Å—Ç—É–ø–Ω—ñ –ö—Ä–æ–∫–∏**:
- Setup GPU infrastructure
- Implement BERT vectorizer –∑ fallback mechanisms
- Conduct 2-week A/B testing
- Monitor ROI closely

#### Scenario C: Mixed Results (0.78 ‚â§ F1 < 0.83)
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è**: Targeted optimization approach
**–ù–∞—Å—Ç—É–ø–Ω—ñ –ö—Ä–æ–∫–∏**:
- Analyze specific failure cases
- Implement hybrid approach (SpaCy + rule-based improvements)
- Consider domain-specific SpaCy model training
- Re-evaluate BERT –ø—ñ—Å–ª—è optimization

---

## –§—ñ–Ω–∞–ª—å–Ω—ñ –°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—ñ –í–∏—Å–Ω–æ–≤–∫–∏

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ Excellence

–¶–µ–π –ø–ª–∞–Ω –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î **exemplary adaptive system design**:

**1. Risk-Minimized Evolution**: –ü–æ—Å—Ç—É–ø–æ–≤–∏–π –ø–µ—Ä–µ—Ö—ñ–¥ –≤—ñ–¥ –ø—Ä–æ—Å—Ç–∏—Ö –¥–æ —Å–∫–ª–∞–¥–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
**2. Data-Driven Decisions**: –ö–æ–∂–µ–Ω –µ–≤–æ–ª—é—Ü—ñ–π–Ω–∏–π –∫—Ä–æ–∫ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏–π –º–µ—Ç—Ä–∏–∫–∞–º–∏
**3. Business-Aligned Development**: Technical decisions tied –¥–æ measurable business outcomes
**4. Infrastructure Flexibility**: Support –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö deployment scenarios
**5. Team-Centric Approach**: Cognitive load management —á–µ—Ä–µ–∑ gradual complexity introduction

### ROI Optimization Strategy

**Phase 1B.1 (SpaCy)**: 
- Investment: ~$20K
- Risk: –ù–∏–∑—å–∫–∏–π
- Expected ROI: 200-300% —á–µ—Ä–µ–∑ recall improvement

**Phase 1B.2 (BERT)**: 
- Additional Investment: ~$25K
- Risk: –°–µ—Ä–µ–¥–Ω—ñ–π  
- Expected ROI: 150-250% –∑–∞ —É–º–æ–≤–∏ performance gap

### Success Factors –¥–ª—è Implementation

**1. Team Preparation**: Ensure team –º–∞—î sufficient NLP expertise
**2. Infrastructure Planning**: GPU access plan –¥–ª—è potential BERT evolution
**3. Business Stakeholder Alignment**: Clear metrics —Ç–∞ expectations
**4. Monitoring Setup**: Comprehensive performance tracking from day 1
**5. Fallback Strategies**: Multiple levels of graceful degradation

### –î–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∞ –°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–∞ –¶—ñ–Ω–Ω—ñ—Å—Ç—å

–¶–µ–π modular evolution approach —Å—Ç–≤–æ—Ä—é—î **sustainable semantic enhancement platform**:
- **Future-Proof Architecture**: Easy integration –Ω–æ–≤–∏—Ö vectorization technologies
- **Cost-Effective Scaling**: Infrastructure investment aligned –∑ proven ROI
- **Knowledge Transfer**: Team develops expertise incrementally
- **Risk Mitigation**: Multiple fallback options preserve system reliability

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è**: PROCEED –∑ implementation. –¶–µ–π –ø–ª–∞–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î optimal balance –º—ñ–∂ innovation —Ç–∞ pragmatism, enabling immediate value delivery –∑ clear evolution pathway.