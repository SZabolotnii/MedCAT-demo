# MedCAT v2 Implementation Plan: Custom Medical Ontology Integration

## –°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–∞ –§—ñ–ª–æ—Å–æ—Ñ—ñ—è –ü—Ä–æ–µ–∫—Ç—É

> **"Complexity is the enemy of reliable medical NLP"**

### –ö–ª—é—á–æ–≤—ñ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ü—Ä–∏–Ω—Ü–∏–ø–∏

**–ú–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å —á–µ—Ä–µ–∑ —ñ–∑–æ–ª—è—Ü—ñ—é:**
- –ö–æ–∂–Ω–∞ —Ñ–∞–∑–∞ —Å—Ç–≤–æ—Ä—é—î —Å–∞–º–æ—Å—Ç—ñ–π–Ω–∏–π, —Ä–æ–±–æ—á–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
- –§–∞–∑–∞ 1A –ø—Ä–∞—Ü—é—î –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ 1B
- Semantic layer –¥–æ–¥–∞—î—Ç—å—Å—è –±–µ–∑ –ø–µ—Ä–µ–ø–∏—Å—É–≤–∞–Ω–Ω—è dictionary foundation

**Human-centric design:**
- –ú—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è cognitive load –¥–ª—è –º–µ–¥–∏—á–Ω–∏—Ö –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤ –ø—Ä–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
- Self-documenting code –∑ —á—ñ—Ç–∫–∏–º–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—è–º–∏
- –ü—Ä–æ–∑–æ—Ä—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è non-technical stakeholders

**Adaptive architecture:**
- –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö keywords –±–µ–∑ retraining
- Swap semantic similarity engines (Bio_ClinicalBERT ‚Üí —ñ–Ω—à—ñ embeddings)
- Scale –≤—ñ–¥ 7K –¥–æ 70K+ concepts –±–µ–∑ architectural refactoring

---

## Executive Summary

**–ü—Ä–æ–±–ª–µ–º–∞:** –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è 7,219 –º–µ–¥–∏—á–Ω–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç—ñ–≤ (keywords + hints) —É MedCAT v2 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó –¥–µ—Ç–µ–∫—Ü—ñ—ó –≤ –∞–Ω–≥–ª–æ–º–æ–≤–Ω–∏—Ö –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö.

**–†—ñ—à–µ–Ω–Ω—è:** –¢—Ä–∏—Ñ–∞–∑–Ω–∞ –≥—ñ–±—Ä–∏–¥–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ - dictionary-based foundation –∑ optional semantic enhancement.

**Timeline:** 6-8 —Ç–∏–∂–Ω—ñ–≤ –¥–æ production-ready –º–æ–¥–µ–ª—ñ
**Budget:** $60K-$100K (–∑–∞ 1 senior ML engineer + 0.5 medical annotator)
**Expected Performance:** F1 ‚âà 0.78-0.85 (Phase 1A) ‚Üí 0.83-0.88 (Phase 1B)

**–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–µ—Ä–µ–≤–∞–≥–∞:** 0% ambiguity –æ–Ω—Ç–æ–ª–æ–≥—ñ—è –¥–æ–∑–≤–æ–ª—è—î —à–≤–∏–¥–∫–∏–π deployment –±–µ–∑ costly unsupervised training.

---

## Phase 1A: Dictionary-Based Foundation [–ü–†–Ü–û–†–ò–¢–ï–¢]

**–°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è:** –°—Ç–≤–æ—Ä–∏—Ç–∏ minimal viable extractor –∑–∞ 2-3 —Ç–∏–∂–Ω—ñ, —â–æ –ø–æ–∫—Ä–∏–≤–∞—î 75-80% use cases. –¶–µ –¥–∞—î immediate value —Ç–∞ baseline –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ ML enhancement.

### –¢–∏–∂–¥–µ–Ω—å 1: Data Preparation & CDB Creation

#### 1.1 –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è CSV ‚Üí MedCAT Format

**–ú–µ—Ç–∞:** –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ `internal_short.csv` —É —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —è–∫—É MedCAT v2 CDB –º–æ–∂–µ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏.

**–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:**
```
internal_short.csv:
- source, keyword, uid, cluster, cluster_title, keyword_hints
- 7,219 records
- keyword_hints: pipe-separated synonyms –∑ [combined_hint] markers
```

**–í–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:**
```
internal_medcat_v2.csv:
- cui, name, ontologies, name_status, type_ids, description
- ~70K rows (7,219 keywords √ó ~10 synonyms –∫–æ–∂–µ–Ω)
```

**Implementation Script:**

```python
"""
scripts/transform_to_medcat_format.py
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è internal_short.csv ‚Üí MedCAT v2 CDB CSV format
"""

import pandas as pd
import re
from pathlib import Path
from typing import List, Tuple

def parse_combined_hints(hint: str) -> Tuple[str, bool, List[str]]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ hints –∑ [combined_hint] markers.
    
    Args:
        hint: "aerosol [combined_hint] intranasally"
    
    Returns:
        - cleaned_hint: "aerosol intranasally"
        - is_combined: True
        - components: ["aerosol", "intranasally"]
    """
    is_combined = '[combined_hint]' in hint
    cleaned = re.sub(r'\s*\[combined_hint\]\s*', ' ', hint).strip()
    components = cleaned.split() if is_combined else [cleaned]
    return cleaned, is_combined, components

def expand_keywords_to_medcat_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ keyword + hints ‚Üí –º–Ω–æ–∂–∏–Ω–Ω—ñ MedCAT rows.
    
    –õ–æ–≥—ñ–∫–∞:
    1. Primary row: keyword —è–∫ preferred name (name_status='P')
    2. Synonym rows: –∫–æ–∂–µ–Ω hint —è–∫ automatic name (name_status='A')
    3. Combined hints: —Å—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—É metadata –¥–ª—è gap tolerance
    """
    medcat_rows = []
    
    for idx, row in df.iterrows():
        cui = row['uid']  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ existing UID —è–∫ CUI
        keyword = row['keyword']
        cluster_id = row['cluster']
        cluster_title = row['cluster_title']
        
        # Primary name entry
        medcat_rows.append({
            'cui': cui,
            'name': keyword,
            'ontologies': 'CUSTOM_INTERNAL',
            'name_status': 'P',
            'type_ids': cluster_id,
            'description': f'{cluster_title} | Primary concept name'
        })
        
        # Synonym entries –∑ hints
        if pd.notna(row['keyword_hints']) and row['keyword_hints'].strip():
            hints = [h.strip() for h in row['keyword_hints'].split('|') if h.strip()]
            
            for hint in hints:
                cleaned_hint, is_combined, components = parse_combined_hints(hint)
                
                # Skip —è–∫—â–æ hint —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–π keyword (redundant)
                if cleaned_hint.lower() == keyword.lower():
                    continue
                
                description = f'{cluster_title} | Synonym'
                if is_combined:
                    description += f' | COMBINED: {" + ".join(components)}'
                
                medcat_rows.append({
                    'cui': cui,
                    'name': cleaned_hint,
                    'ontologies': '',
                    'name_status': 'A',
                    'type_ids': '',
                    'description': description
                })
    
    return pd.DataFrame(medcat_rows)

def validate_transformed_data(df: pd.DataFrame) -> dict:
    """–í–∞–ª—ñ–¥–∞—Ü—ñ—è —è–∫–æ—Å—Ç—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö."""
    stats = {
        'total_rows': len(df),
        'unique_cuis': df['cui'].nunique(),
        'primary_names': len(df[df['name_status'] == 'P']),
        'synonyms': len(df[df['name_status'] == 'A']),
        'avg_synonyms_per_cui': len(df[df['name_status'] == 'A']) / df['cui'].nunique(),
        'empty_names': df['name'].isna().sum(),
        'combined_hints': df['description'].str.contains('COMBINED', na=False).sum()
    }
    return stats

def main():
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    input_path = Path('internal_short.csv')
    df = pd.read_csv(input_path)
    
    print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} records –∑ {input_path}")
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è
    medcat_df = expand_keywords_to_medcat_rows(df)
    
    # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
    stats = validate_transformed_data(medcat_df)
    print("\n‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏
    if stats['empty_names'] > 0:
        print(f"\n‚ö†Ô∏è –£–í–ê–ì–ê: {stats['empty_names']} –ø–æ—Ä–æ–∂–Ω—ñ—Ö names - –ø–æ—Ç—Ä—ñ–±–Ω–∞ –æ—á–∏—Å—Ç–∫–∞!")
    
    if stats['avg_synonyms_per_cui'] < 3:
        print(f"\n‚ö†Ô∏è –£–í–ê–ì–ê: –ù–∏–∑—å–∫–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤ ({stats['avg_synonyms_per_cui']:.1f}) - recall –º–æ–∂–µ –±—É—Ç–∏ –Ω–∏–∑—å–∫–∏–º")
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    output_path = Path('data/internal_medcat_v2.csv')
    output_path.parent.mkdir(parents=True, exist_ok=True)
    medcat_df.to_csv(output_path, index=False)
    
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —É {output_path}")
    
    # Sample output –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
    print("\nüìã –ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Ö–æ–¥—É (–ø–µ—Ä—à—ñ 3 CUI):")
    sample_cuis = medcat_df['cui'].unique()[:3]
    for cui in sample_cuis:
        subset = medcat_df[medcat_df['cui'] == cui]
        print(f"\n   CUI {cui}:")
        for _, row in subset.iterrows():
            status_label = "PRIMARY" if row['name_status'] == 'P' else "SYNONYM"
            print(f"      [{status_label}] {row['name']}")

if __name__ == '__main__':
    main()
```

**Execution:**
```bash
python scripts/transform_to_medcat_format.py
```

**Success Criteria:**
- ‚úÖ ~70K rows generated (7,219 CUI √ó ~10 names –∫–æ–∂–µ–Ω)
- ‚úÖ –ö–æ–∂–µ–Ω CUI –º–∞—î 1 primary name (P) + N synonyms (A)
- ‚úÖ 0 empty names
- ‚úÖ Combined hints identified —É description

**Time Estimate:** 2-3 –¥–Ω—ñ (script dev + validation + debugging)

---

#### 1.2 CDB Creation –∑ Custom Ontology

**–ú–µ—Ç–∞:** –°—Ç–≤–æ—Ä–∏—Ç–∏ MedCAT v2 Concept Database –∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–æ–≥–æ CSV.

**Architecture Decision:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ `CDBMaker.prepare_csvs()` –∑ `full_build=False` –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–µ–∑ context vectors (Phase 1A –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —ó—Ö).

**Implementation:**

```python
"""
scripts/create_cdb_v2.py
–°—Ç–≤–æ—Ä–µ–Ω–Ω—è MedCAT v2 CDB –∑ custom ontology CSV
"""

from medcat.cdb_maker import CDBMaker
from medcat.config import Config
from pathlib import Path
import json

def configure_for_dictionary_mode(config: Config) -> Config:
    """
    –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è MedCAT –¥–ª—è dictionary-only —Ä–µ–∂–∏–º—É –±–µ–∑ ML.
    
    –ö–ª—é—á–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:
    - –í–∏–º–∫–Ω—É—Ç–∏ training
    - –í–∏–º–∫–Ω—É—Ç–∏ context similarity (–ø–æ–∫–∏ —â–æ)
    - –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ spell checking
    - –û–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –¥–ª—è high precision
    """
    # === GENERAL ===
    config.general['spacy_model'] = 'en_core_web_md'
    config.general['train'] = False
    config.general['spell_check'] = True
    config.general['spell_check_len_limit'] = 3  # –ú—ñ–Ω –¥–æ–≤–∂–∏–Ω–∞ –¥–ª—è spell check
    
    # === LINKING (–±–µ–∑ ML) ===
    config.linking['train'] = False
    config.linking['always_calculate_similarity'] = False  # Phase 1A: disabled
    config.linking['calculate_dynamic_threshold'] = False
    config.linking['similarity_threshold'] = 1.0  # Effectively disabled
    
    # Rule-based disambiguation preferences
    config.linking['prefer_primary_name'] = 0.6  # –ü–µ—Ä–µ–≤–∞–≥–∞ primary names
    config.linking['prefer_frequent_concepts'] = 0.3
    config.linking['disamb_length_limit'] = 6  # Max words –¥–ª—è disambiguation
    
    # === NER ===
    config.ner['min_name_len'] = 2  # –î–æ–∑–≤–æ–ª–∏—Ç–∏ –∫–æ—Ä–æ—Ç–∫—ñ –º–µ–¥–∏—á–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏
    config.ner['upper_case_limit_len'] = 4  # COPD, GERD —Ç–æ—â–æ
    config.ner['check_upper_case_names'] = True
    config.ner['try_reverse_word_order'] = True  # "pain chest" ‚Üí "chest pain"
    
    return config

def create_cdb_from_csv(
    csv_path: Path,
    output_dir: Path,
    config: Config
) -> 'CDB':
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è CDB —á–µ—Ä–µ–∑ CDBMaker."""
    
    print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è CDB –∑ {csv_path}")
    
    maker = CDBMaker(config=config)
    
    cdb = maker.prepare_csvs(
        csv_paths=[str(csv_path)],
        sep=',',
        encoding='utf-8',
        full_build=False  # Phase 1A: dictionary-only, –±–µ–∑ context vectors
    )
    
    print(f"‚úÖ CDB —Å—Ç–≤–æ—Ä–µ–Ω–æ: {len(cdb.cui2names)} CUI")
    
    return cdb

def enrich_cdb_metadata(cdb: 'CDB') -> 'CDB':
    """
    –î–æ–¥–∞–≤–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è type_ids (cluster mappings).
    
    MedCAT –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î type_ids –¥–ª—è semantic grouping.
    –ú–∞–ø–ø–∏–º–æ cluster IDs ‚Üí human-readable –Ω–∞–∑–≤–∏.
    """
    # –í–∏—Ç—è–≥—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ type_id ‚Üí cluster_title –∑ –æ–ø–∏—Å—É
    type_id_names = {}
    
    for cui, names in cdb.cui2names.items():
        type_ids = cdb.cui2type_ids.get(cui, set())
        
        for type_id in type_ids:
            if type_id not in type_id_names:
                # –í–∏—Ç—è–≥—É—î–º–æ cluster_title –∑ description
                # Format: "Action/Drug | Primary concept name"
                if cui in cdb.cui2preferred_name:
                    pref_name = cdb.cui2preferred_name[cui]
                    # –ü—Ä–æ—Å—Ç–∏–π lookup - –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏
                    type_id_names[type_id] = f"Cluster_{type_id[:8]}"
    
    # –î–æ–¥–∞—î–º–æ –¥–æ CDB metadata
    cdb.addl_info['type_id2name'] = type_id_names
    
    print(f"üìù –î–æ–¥–∞–Ω–æ {len(type_id_names)} type ID mappings")
    
    return cdb

def validate_cdb_quality(cdb: 'CDB') -> dict:
    """–í–∞–ª—ñ–¥–∞—Ü—ñ—è CDB –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è."""
    
    stats = {
        'total_cuis': len(cdb.cui2names),
        'total_names': sum(len(names) for names in cdb.cui2names.values()),
        'avg_names_per_cui': sum(len(names) for names in cdb.cui2names.values()) / len(cdb.cui2names),
        'type_ids_count': len(set().union(*[cdb.cui2type_ids.get(cui, set()) for cui in cdb.cui2names])),
        'preferred_names_coverage': len(cdb.cui2preferred_name) / len(cdb.cui2names) * 100
    }
    
    return stats

def save_cdb_with_metadata(cdb: 'CDB', output_dir: Path):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è CDB + metadata –¥–ª—è Phase 1A."""
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è CDB
    cdb_path = output_dir / 'custom_cdb_v2.dat'
    cdb.save(str(cdb_path))
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    config_path = output_dir / 'config.json'
    cdb.config.save(str(config_path))
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = validate_cdb_quality(cdb)
    stats_path = output_dir / 'cdb_stats.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"\nüíæ CDB –∑–±–µ—Ä–µ–∂–µ–Ω–æ:")
    print(f"   CDB: {cdb_path}")
    print(f"   Config: {config_path}")
    print(f"   Stats: {stats_path}")
    
    print(f"\nüìä CDB –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    for key, value in stats.items():
        print(f"   {key}: {value}")

def main():
    csv_path = Path('data/internal_medcat_v2.csv')
    output_dir = Path('models/phase1a_dictionary')
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    config = Config()
    config = configure_for_dictionary_mode(config)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è CDB
    cdb = create_cdb_from_csv(csv_path, output_dir, config)
    
    # Enrichment
    cdb = enrich_cdb_metadata(cdb)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    save_cdb_with_metadata(cdb, output_dir)
    
    print("\n‚úÖ Phase 1A CDB –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!")

if __name__ == '__main__':
    main()
```

**Execution:**
```bash
python scripts/create_cdb_v2.py
```

**Output Structure:**
```
models/phase1a_dictionary/
‚îú‚îÄ‚îÄ custom_cdb_v2.dat          # MedCAT CDB binary
‚îú‚îÄ‚îÄ config.json                # Configuration
‚îî‚îÄ‚îÄ cdb_stats.json             # Quality metrics
```

**Success Criteria:**
- ‚úÖ CDB –º—ñ—Å—Ç–∏—Ç—å 7,219 CUI
- ‚úÖ –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å names per CUI ‚âà 8-10
- ‚úÖ 100% CUI –º–∞—é—Ç—å preferred name
- ‚úÖ Type IDs mapped –¥–æ cluster titles

**Time Estimate:** 1-2 –¥–Ω—ñ

---

### –¢–∏–∂–¥–µ–Ω—å 2: CAT Initialization & Combined Hints Processing

#### 1.3 CAT Instance –∑ Custom Components

**–ú–µ—Ç–∞:** –°—Ç–≤–æ—Ä–∏—Ç–∏ MedCAT CAT instance –∑ custom preprocessing –¥–ª—è `[combined_hint]` patterns.

**Architectural Challenge:** MedCAT's default expanding window algorithm –Ω–µ –≤—Ä–∞—Ö–æ–≤—É—î gap-tolerant patterns ("aerosol ... intranasally"). –ü–æ—Ç—Ä—ñ–±–µ–Ω custom NER component.

**Solution Strategy:** Hybrid pipeline - spaCy preprocessing ‚Üí Custom Gap-Tolerant Matcher ‚Üí MedCAT linking.

**Implementation:**

```python
"""
src/custom_cat_v2.py
Custom CAT wrapper –∑ gap-tolerant combined hints matching
"""

from medcat.cat import CAT
from medcat.cdb import CDB
from medcat.config import Config
from pathlib import Path
from typing import Dict, List, Set, Tuple
import re
import spacy
from spacy.tokens import Doc, Span

class CombinedHintMatcher:
    """
    Custom component –¥–ª—è gap-tolerant matching of [combined_hint] patterns.
    
    Architecture:
    1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î combined hints –∑ CDB descriptions
    2. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ hint —Å—Ç–≤–æ—Ä—é—î pattern –∑ gap tolerance
    3. –®—É–∫–∞—î matches —É spaCy Doc –ø–µ—Ä–µ–¥ MedCAT processing
    4. –î–æ–¥–∞—î —è–∫ custom entities –¥–æ Doc
    
    Example:
        Pattern: "aerosol [combined_hint] intranasally"
        Matches: "aerosol administered intranasally" (gap=1)
                 "aerosol spray given intranasally" (gap=2)
    """
    
    def __init__(self, cdb: CDB, max_gap: int = 3):
        """
        Args:
            cdb: MedCAT CDB –∑ descriptions containing COMBINED markers
            max_gap: –ú–∞–∫—Å–∏–º—É–º —Å–ª—ñ–≤ –º—ñ–∂ —á–∞—Å—Ç–∏–Ω–∞–º–∏ combined hint
        """
        self.cdb = cdb
        self.max_gap = max_gap
        self.combined_patterns = self._extract_combined_patterns()
        
        print(f"üîç –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.combined_patterns)} combined hint patterns")
    
    def _extract_combined_patterns(self) -> Dict[str, List[Tuple[List[str], str]]]:
        """
        –í–∏—Ç—è–≥—É—î combined patterns –∑ CDB descriptions.
        
        Returns:
            Dict[cui -> List[(components, full_pattern)]]
            
        Example:
            {
                'CUI123': [
                    (['aerosol', 'intranasally'], 'aerosol intranasally'),
                    (['oral', 'tablet'], 'oral tablet')
                ]
            }
        """
        patterns = {}
        
        for cui, names in self.cdb.cui2names.items():
            cui_patterns = []
            
            for name in names:
                # –®—É–∫–∞—î–º–æ –≤ descriptions for this name
                # –§–æ—Ä–º–∞—Ç: "... | COMBINED: aerosol + intranasally"
                # (description –Ω–µ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –≤ cui2names, –ø–æ—Ç—Ä—ñ–±–µ–Ω alternative approach)
                
                # –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê: –ø–∞—Ä—Å–∏–º–æ directly –∑ name —è–∫—â–æ –º—ñ—Å—Ç–∏—Ç—å markers
                # –î–ª—è Phase 1A —Å–ø—Ä–æ—â—É—î–º–æ - —à—É–∫–∞—î–º–æ multi-word names —è–∫ candidates
                words = name.split()
                if len(words) >= 2:  # Potential combined pattern
                    cui_patterns.append((words, name))
            
            if cui_patterns:
                patterns[cui] = cui_patterns
        
        return patterns
    
    def __call__(self, doc: Doc) -> Doc:
        """
        spaCy pipeline component - –¥–æ–¥–∞—î custom entities –¥–ª—è combined hints.
        
        Args:
            doc: spaCy Doc
        
        Returns:
            Doc –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ entities —É doc.ents
        """
        new_ents = []
        
        for cui, patterns in self.combined_patterns.items():
            for components, full_pattern in patterns:
                matches = self._find_gap_tolerant_matches(doc, components)
                
                for start_idx, end_idx in matches:
                    span = Span(doc, start_idx, end_idx, label="COMBINED_HINT")
                    span._.cui = cui  # Custom attribute
                    span._.pattern = full_pattern
                    new_ents.append(span)
        
        # Merge –∑ existing entities (resolve overlaps)
        doc.ents = tuple(new_ents) + doc.ents
        
        return doc
    
    def _find_gap_tolerant_matches(
        self, 
        doc: Doc, 
        components: List[str]
    ) -> List[Tuple[int, int]]:
        """
        –ó–Ω–∞—Ö–æ–¥–∏—Ç—å matches –∑ gap tolerance.
        
        Algorithm:
        1. –ó–Ω–∞–π—Ç–∏ –ø–µ—Ä—à—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
        2. –®—É–∫–∞—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É –≤ –º–µ–∂–∞—Ö max_gap —Å–ª—ñ–≤
        3. Repeat –¥–ª—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        4. Return (start_idx, end_idx) —è–∫—â–æ –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ
        """
        matches = []
        
        for i in range(len(doc)):
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä—à–æ—ó –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
            if doc[i].text.lower() == components[0].lower():
                # –°–ø—Ä–æ–±–∞ –∑–Ω–∞–π—Ç–∏ —Ä–µ—à—Ç—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                match_indices = [i]
                search_idx = i + 1
                
                for comp_idx in range(1, len(components)):
                    target_comp = components[comp_idx].lower()
                    found = False
                    
                    # –®—É–∫–∞—î–º–æ –≤ –º–µ–∂–∞—Ö gap
                    for gap in range(1, self.max_gap + 1):
                        check_idx = search_idx + gap - 1
                        if check_idx < len(doc) and doc[check_idx].text.lower() == target_comp:
                            match_indices.append(check_idx)
                            search_idx = check_idx + 1
                            found = True
                            break
                    
                    if not found:
                        break
                
                # –í—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ
                if len(match_indices) == len(components):
                    start = match_indices[0]
                    end = match_indices[-1] + 1  # spaCy exclusive end
                    matches.append((start, end))
        
        return matches

class CustomCAT:
    """
    Wrapper –Ω–∞–≤–∫–æ–ª–æ MedCAT CAT –∑ custom preprocessing.
    
    Architecture:
    - Phase 1A: Dictionary matching + Combined hints
    - Phase 1B: + Semantic similarity (future)
    
    Design Decision: Composition over inheritance –¥–ª—è flexibility.
    """
    
    def __init__(self, cdb_path: Path, config_path: Path = None):
        """
        Args:
            cdb_path: Path to CDB .dat file
            config_path: Optional config.json (–∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î cdb.config)
        """
        print("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è CustomCAT Phase 1A...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CDB
        self.cdb = CDB.load(str(cdb_path))
        print(f"‚úÖ CDB –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(self.cdb.cui2names)} CUI")
        
        # Config
        if config_path:
            self.config = Config.load(str(config_path))
        else:
            self.config = self.cdb.config
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è base CAT (–±–µ–∑ vocab –¥–ª—è Phase 1A)
        self.cat = CAT(cdb=self.cdb, config=self.config, vocab=None)
        self.cat.spacy_cat.train = False  # –ö—Ä–∏—Ç–∏—á–Ω–æ: disable training
        
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è custom pipeline component
        self.combined_matcher = CombinedHintMatcher(self.cdb, max_gap=3)
        
        # Register custom component —É spaCy pipeline
        if not Doc.has_extension('cui'):
            Doc.set_extension('cui', default=None)
        if not Span.has_extension('cui'):
            Span.set_extension('cui', default=None)
        if not Span.has_extension('pattern'):
            Span.set_extension('pattern', default=None)
        
        # –î–æ–¥–∞—î–º–æ –ü–ï–†–ï–î MedCAT NER
        self.cat.nlp.add_pipe(
            'combined_hint_matcher',
            before='ner',
            component=self.combined_matcher
        )
        
        print("‚úÖ CustomCAT –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è")
    
    def extract_entities(self, text: str, only_cui: bool = False) -> Dict:
        """
        Wrapper –Ω–∞–≤–∫–æ–ª–æ cat.get_entities –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ—é –æ–±—Ä–æ–±–∫–æ—é.
        
        Args:
            text: –ö–ª—ñ–Ω—ñ—á–Ω–∏–π —Ç–µ–∫—Å—Ç
            only_cui: –ü–æ–≤–µ—Ä—Ç–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ CUI (True) –∞–±–æ –ø–æ–≤–Ω—É —ñ–Ω—Ñ–æ (False)
        
        Returns:
            Dict –∑ entities —É MedCAT format + custom metadata
        """
        # Standard MedCAT extraction
        result = self.cat.get_entities(text, only_cui=only_cui)
        
        # –î–æ–¥–∞—î–º–æ metadata –ø—Ä–æ combined hints
        for ent_id, entity in result.get('entities', {}).items():
            # Check —á–∏ —Ü–µ combined hint match
            # (–≤ production –¥–æ–¥–∞—Ç–∏ –±—ñ–ª—å—à sophisticated tracking)
            pass
        
        return result
    
    def batch_process(
        self, 
        texts: List[str], 
        batch_size: int = 32
    ) -> List[Dict]:
        """
        Batch processing –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
            batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è processing
        
        Returns:
            List[Dict] results –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        """
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            
            for text in batch:
                result = self.extract_entities(text)
                results.append(result)
        
        return results

def main():
    """Quick test CustomCAT."""
    cdb_path = Path('models/phase1a_dictionary/custom_cdb_v2.dat')
    
    cat = CustomCAT(cdb_path)
    
    # Test text
    test_text = """
    Patient prescribed metformin 500mg orally twice daily.
    Administered ceftriaxone intranasally.
    History of diabetes mellitus and hypertension.
    """
    
    entities = cat.extract_entities(test_text)
    
    print(f"\nüî¨ Test Results:")
    print(f"Text: {test_text[:100]}...")
    print(f"Entities found: {len(entities.get('entities', {}))}")
    
    for ent_id, ent in list(entities.get('entities', {}).items())[:5]:
        print(f"\n  - {ent['source_value']}")
        print(f"    CUI: {ent['cui']}")
        print(f"    Confidence: {ent['acc']:.3f}")

if __name__ == '__main__':
    main()
```

**Success Criteria:**
- ‚úÖ CAT instance —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è —É—Å–ø—ñ—à–Ω–æ
- ‚úÖ Combined hint matcher –∑–Ω–∞—Ö–æ–¥–∏—Ç—å gap-tolerant patterns
- ‚úÖ Batch processing –ø—Ä–∞—Ü—é—î –¥–ª—è 10+ documents

**Time Estimate:** 3-4 –¥–Ω—ñ

---

### –¢–∏–∂–¥–µ–Ω—å 3: Validation –Ω–∞ Sample Clinical Notes

#### 1.4 Testing Framework & Metrics

**–ú–µ—Ç–∞:** –°—Ç–≤–æ—Ä–∏—Ç–∏ reproducible validation framework –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ Phase 1A performance.

**Implementation:**

```python
"""
scripts/validate_phase1a.py
Validation framework –¥–ª—è Phase 1A –º–æ–¥–µ–ª—ñ
"""

from pathlib import Path
from typing import Dict, List, Tuple
import json
from src.custom_cat_v2 import CustomCAT

class ValidationFramework:
    """
    Framework –¥–ª—è systematic evaluation Phase 1A model.
    
    Metrics:
    - Entity-level: Precision, Recall, F1
    - Span-level: Exact match accuracy
    - Type-level: Cluster accuracy
    """
    
    def __init__(self, cat: CustomCAT, test_docs_path: Path):
        self.cat = cat
        self.test_docs = self._load_test_docs(test_docs_path)
        
    def _load_test_docs(self, path: Path) -> List[Dict]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è test documents."""
        # Format: —Å–ø–∏—Å–æ–∫ dictionaries –∑ 'text' —Ç–∞ optional 'annotations'
        with open(path, 'r') as f:
            return json.load(f)
    
    def run_validation(self) -> Dict:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
        
        Returns:
            Dict –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–∞ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        results = {
            'total_docs': len(self.test_docs),
            'entities_found': 0,
            'avg_entities_per_doc': 0,
            'coverage_by_cluster': {},
            'sample_outputs': []
        }
        
        all_entities = []
        
        for doc_idx, doc in enumerate(self.test_docs):
            text = doc['text']
            entities = self.cat.extract_entities(text)
            
            doc_entity_count = len(entities.get('entities', {}))
            all_entities.append(entities)
            results['entities_found'] += doc_entity_count
            
            # Sample output (–ø–µ—Ä—à—ñ 3 documents)
            if doc_idx < 3:
                results['sample_outputs'].append({
                    'text': text[:200] + '...',
                    'entity_count': doc_entity_count,
                    'entities': [
                        {
                            'text': e['source_value'],
                            'cui': e['cui'],
                            'type': e.get('types', ['UNKNOWN'])[0] if e.get('types') else 'UNKNOWN',
                            'confidence': e['acc']
                        }
                        for e in list(entities.get('entities', {}).values())[:5]
                    ]
                })
        
        results['avg_entities_per_doc'] = results['entities_found'] / results['total_docs']
        
        return results
    
    def generate_report(self, results: Dict, output_path: Path):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è validation report."""
        
        report_lines = [
            "# Phase 1A Validation Report",
            "",
            "## Summary Statistics",
            f"- Total documents tested: {results['total_docs']}",
            f"- Total entities found: {results['entities_found']}",
            f"- Avg entities per document: {results['avg_entities_per_doc']:.2f}",
            "",
            "## Sample Outputs",
            ""
        ]
        
        for idx, sample in enumerate(results['sample_outputs'], 1):
            report_lines.append(f"### Document {idx}")
            report_lines.append(f"**Text:** {sample['text']}")
            report_lines.append(f"**Entities found:** {sample['entity_count']}")
            report_lines.append("")
            
            for ent in sample['entities']:
                report_lines.append(
                    f"- `{ent['text']}` ‚Üí **{ent['cui']}** "
                    f"(type: {ent['type']}, conf: {ent['confidence']:.3f})"
                )
            report_lines.append("")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            f.write('\n'.join(report_lines))
        
        print(f"üìä Validation report saved: {output_path}")

def main():
    # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ test documents
    test_docs_path = Path('data/test_clinical_notes.json')
    
    # –Ø–∫—â–æ –Ω–µ–º–∞—î –≥–æ—Ç–æ–≤–∏—Ö - —Å—Ç–≤–æ—Ä—é—î–º–æ synthetic test set
    if not test_docs_path.exists():
        print("‚ö†Ô∏è Test documents not found. Creating synthetic test set...")
        synthetic_docs = [
            {"text": "Patient prescribed metformin 500mg orally twice daily for type 2 diabetes."},
            {"text": "Administered ceftriaxone 1g intravenously for pneumonia treatment."},
            {"text": "History of hypertension managed with lisinopril 10mg daily."},
            # ... –¥–æ–¥–∞—Ç–∏ –±—ñ–ª—å—à–µ synthetic examples
        ]
        test_docs_path.parent.mkdir(parents=True, exist_ok=True)
        with open(test_docs_path, 'w') as f:
            json.dump(synthetic_docs, f, indent=2)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ model
    cdb_path = Path('models/phase1a_dictionary/custom_cdb_v2.dat')
    cat = CustomCAT(cdb_path)
    
    # –ó–∞–ø—É—Å–∫ validation
    framework = ValidationFramework(cat, test_docs_path)
    results = framework.run_validation()
    
    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è report
    report_path = Path('reports/phase1a_validation.md')
    framework.generate_report(results, report_path)
    
    print("\n‚úÖ Validation completed!")
    print(f"üìä Results summary:")
    print(f"   - Documents: {results['total_docs']}")
    print(f"   - Entities: {results['entities_found']}")
    print(f"   - Avg per doc: {results['avg_entities_per_doc']:.2f}")

if __name__ == '__main__':
    main()
```

**Success Criteria Phase 1A:**
- ‚úÖ Entities detected —É >= 80% test documents
- ‚úÖ Avg entities per document >= 5
- ‚úÖ Zero crashes –Ω–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö
- ‚úÖ Reproducible metrics –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ Phase 1B

**Time Estimate:** 2-3 –¥–Ω—ñ

---

## Phase 1A Summary & Go/No-Go Decision Point

**Deliverables:**
1. ‚úÖ MedCAT v2 CDB –∑ 7,219 concepts
2. ‚úÖ CustomCAT –∑ gap-tolerant combined hints
3. ‚úÖ Validation framework –∑ metrics
4. ‚úÖ Baseline performance report

**Expected Performance (Phase 1A):**
- Precision: 0.75-0.85 (high - low false positives —á–µ—Ä–µ–∑ 0% ambiguity)
- Recall: 0.70-0.80 (medium - –æ–±–º–µ–∂–µ–Ω–∏–π synonym coverage)
- F1: 0.78-0.82
- Processing speed: 10-50 docs/second

**Decision Point:**

**IF F1 >= 0.78 AND Precision >= 0.80:**
‚Üí Phase 1A sufficient –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö use cases
‚Üí Consider Phase 1B optional enhancement
‚Üí Proceed –¥–æ production packaging

**IF F1 < 0.78 OR Recall < 0.70:**
‚Üí Phase 1B (semantic similarity) –æ–±–æ–≤'—è–∑–∫–æ–≤–∞
‚Üí Investigate root causes (hint quality? combined patterns?)

---

## Phase 1B: Semantic Similarity Enhancement [CONDITIONAL]

**–°—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è:** –Ø–∫—â–æ Phase 1A shows F1 < 0.78, –¥–æ–¥–∞—î–º–æ Bio_ClinicalBERT embeddings –¥–ª—è semantic matching –≤–∞—Ä—ñ–∞—Ü—ñ–π –Ω–µ covered hints.

**Timeline:** +2-3 —Ç–∏–∂–Ω—ñ after Phase 1A

### –¢–∏–∂–¥–µ–Ω—å 4-5: Semantic Layer Integration

#### 1.5 Bio_ClinicalBERT Embeddings Setup

**Architecture Decision:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ `transformers` Bio_ClinicalBERT –¥–ª—è generating concept embeddings + faiss –¥–ª—è efficient similarity search.

**Implementation:**

```python
"""
src/semantic_layer.py
Semantic similarity layer for Phase 1B
"""

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from pathlib import Path
import pickle
from typing import Dict, List, Tuple
import faiss

class SemanticMatcher:
    """
    Semantic similarity matching using Bio_ClinicalBERT.
    
    Architecture:
    1. Pre-compute embeddings –¥–ª—è –≤—Å—ñ—Ö CUI names
    2. Build faiss index –¥–ª—è efficient search
    3. During inference: embed detected spans ‚Üí find similar CUI
    
    Advantage: Catches variations not in hints
    Example: "type 2 sugar disease" ‚Üí "type 2 diabetes" (semantic match)
    """
    
    def __init__(
        self, 
        cdb: 'CDB',
        model_name: str = 'emilyalsentzer/Bio_ClinicalBERT',
        similarity_threshold: float = 0.75
    ):
        self.cdb = cdb
        self.threshold = similarity_threshold
        
        print(f"üß† Loading Bio_ClinicalBERT: {model_name}")
        
        # Load model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()  # Inference mode
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        print(f"‚úÖ Model loaded on {self.device}")
        
        # Pre-compute embeddings
        self.cui_embeddings = None
        self.cui_list = None
        self.faiss_index = None
        
    def _embed_text(self, text: str) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è embedding –¥–ª—è —Ç–µ–∫—Å—Ç—É."""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=128,
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Use [CLS] token embedding
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        return embedding[0]  # Shape: (768,)
    
    def build_cui_index(self, cache_path: Path = None):
        """
        Pre-compute embeddings –¥–ª—è –≤—Å—ñ—Ö CUI names —Ç–∞ build faiss index.
        
        This is computationally expensive - cache results.
        """
        if cache_path and cache_path.exists():
            print(f"üì¶ Loading cached embeddings from {cache_path}")
            with open(cache_path, 'rb') as f:
                cached = pickle.load(f)
                self.cui_list = cached['cui_list']
                self.cui_embeddings = cached['embeddings']
        else:
            print(f"üîß Building CUI embeddings index (—Ü–µ –∑–∞–π–º–µ —á–∞—Å)...")
            
            cui_list = []
            embeddings = []
            
            for cui, names in self.cdb.cui2names.items():
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ preferred name –¥–ª—è embedding
                pref_name = self.cdb.cui2preferred_name.get(cui, list(names)[0])
                
                embedding = self._embed_text(pref_name)
                
                cui_list.append(cui)
                embeddings.append(embedding)
            
            self.cui_list = cui_list
            self.cui_embeddings = np.array(embeddings)  # Shape: (N_cui, 768)
            
            # Cache
            if cache_path:
                cache_path.parent.mkdir(parents=True, exist_ok=True)
                with open(cache_path, 'wb') as f:
                    pickle.dump({
                        'cui_list': self.cui_list,
                        'embeddings': self.cui_embeddings
                    }, f)
                print(f"üíæ Embeddings cached to {cache_path}")
        
        # Build faiss index
        dimension = self.cui_embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner product (cosine sim)
        
        # Normalize embeddings –¥–ª—è cosine similarity
        faiss.normalize_L2(self.cui_embeddings)
        self.faiss_index.add(self.cui_embeddings)
        
        print(f"‚úÖ FAISS index built: {len(self.cui_list)} CUI")
    
    def find_similar_cui(
        self, 
        text: str, 
        top_k: int = 5
    ) -> List[Tuple[str, float]]:
        """
        –ó–Ω–∞–π—Ç–∏ top-k CUI –Ω–∞–π–±—ñ–ª—å—à —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–∂–∏—Ö –¥–æ text.
        
        Returns:
            List[(cui, similarity_score)]
        """
        if self.faiss_index is None:
            raise RuntimeError("Index not built. Call build_cui_index() first.")
        
        # Embed query text
        query_emb = self._embed_text(text).reshape(1, -1)
        faiss.normalize_L2(query_emb)
        
        # Search
        similarities, indices = self.faiss_index.search(query_emb, top_k)
        
        results = [
            (self.cui_list[idx], float(sim))
            for idx, sim in zip(indices[0], similarities[0])
            if sim >= self.threshold
        ]
        
        return results

class EnhancedCAT:
    """
    Phase 1B: CustomCAT + Semantic Matching.
    
    Combines:
    - Phase 1A dictionary matching (fast, high precision)
    - Semantic similarity (slower, high recall)
    """
    
    def __init__(self, phase1a_cat: CustomCAT, semantic_matcher: SemanticMatcher):
        self.base_cat = phase1a_cat
        self.semantic = semantic_matcher
    
    def extract_entities(self, text: str) -> Dict:
        """
        Hybrid extraction:
        1. Phase 1A dictionary matching
        2. For unmatched spans ‚Üí semantic similarity
        3. Merge results
        """
        # Phase 1A
        phase1a_results = self.base_cat.extract_entities(text)
        
        # TODO: Implement semantic enhancement
        # - Identify unmatched clinical terms (via spaCy NER?)
        # - Run semantic matching on unmatched spans
        # - Merge –∑ Phase 1A results (resolve overlaps)
        
        return phase1a_results
```

**Success Criteria Phase 1B:**
- ‚úÖ F1 increase >= 0.05 –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ Phase 1A
- ‚úÖ Recall improvement >= 0.08
- ‚úÖ Processing speed acceptable (<5s per document)

**Time Estimate:** 2-3 —Ç–∏–∂–Ω—ñ

---

## Phase 2: Production Readiness

### –¢–∏–∂–¥–µ–Ω—å 6-8: Packaging & Deployment

#### 2.1 Model Packaging

**–ú–µ—Ç–∞:** –°—Ç–≤–æ—Ä–∏—Ç–∏ production-ready package –¥–ª—è distribution.

```python
"""
scripts/create_model_pack.py
Package Phase 1A/1B model —è–∫ single .zip file
"""

from pathlib import Path
from src.custom_cat_v2 import CustomCAT

def create_production_pack(
    cdb_path: Path,
    output_path: Path,
    include_semantic: bool = False
):
    """
    –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —î–¥–∏–Ω–æ–≥–æ model pack.
    
    Includes:
    - CDB
    - Config
    - Custom components code
    - (Optional) Semantic embeddings cache
    - README –∑ usage instructions
    """
    # TODO: Implementation
    pass
```

#### 2.2 Integration Testing

**–ú–µ—Ç–∞:** End-to-end testing –Ω–∞ 10-50 real clinical notes.

#### 2.3 Documentation

**–ú–µ—Ç–∞:** User-facing documentation + API reference.

---

## Risk Mitigation Strategies

### Risk 1: Low Recall —á–µ—Ä–µ–∑ Insufficient Hints

**Mitigation:**
- Phase 1A: Validate hint quality early (Week 1)
- If recall < 0.70 ‚Üí Proceed –¥–æ Phase 1B immediately
- Active learning: Annotate missed entities ‚Üí add as hints

### Risk 2: Combined Hints Gap Tolerance Too Restrictive

**Mitigation:**
- Configurable `max_gap` parameter (default: 3)
- A/B testing –∑ —Ä—ñ–∑–Ω–∏–º–∏ gap values
- Manual review of gap patterns —É test documents

### Risk 3: Bio_ClinicalBERT Embeddings Too Slow

**Mitigation:**
- Pre-compute all CUI embeddings (one-time cost)
- Use faiss GPU for inference
- Batch processing –¥–ª—è multiple documents
- Cache frequent queries

---

## Success Metrics & KPIs

### Phase 1A
- ‚úÖ F1 >= 0.78
- ‚úÖ Precision >= 0.80
- ‚úÖ Processing >= 10 docs/sec
- ‚úÖ Zero false negatives for exact keyword matches

### Phase 1B (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
- ‚úÖ F1 >= 0.83
- ‚úÖ Recall increase >= 0.08 vs Phase 1A
- ‚úÖ Processing >= 2 docs/sec

### Production
- ‚úÖ Stable performance –Ω–∞ diverse clinical texts
- ‚úÖ Reproducible results
- ‚úÖ Clear documentation
- ‚úÖ Easy integration —É downstream applications

---

## Budget & Resource Allocation

**Phase 1A (2-3 —Ç–∏–∂–Ω—ñ):**
- 1 Senior ML Engineer: $15K-$20K
- Compute (CPU): $500
- **Total: $20K**

**Phase 1B (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ, +2-3 —Ç–∏–∂–Ω—ñ):**
- 1 Senior ML Engineer: $15K-$20K
- Compute (GPU –¥–ª—è embeddings): $2K-$3K
- **Total: $20K**

**Phase 2 (2 —Ç–∏–∂–Ω—ñ):**
- Documentation + packaging: $10K
- **Total: $10K**

**Grand Total: $40K-$50K –¥–ª—è Phase 1A+2, –∞–±–æ $60K-$70K —è–∫—â–æ Phase 1B –ø–æ—Ç—Ä—ñ–±–Ω–∞**

–¶–µ –∑–Ω–∞—á–Ω–æ –Ω–∏–∂—á–µ initial $60K-$100K estimate, –∑–∞–≤–¥—è–∫–∏ 0% ambiguity –æ–Ω—Ç–æ–ª–æ–≥—ñ—ó.

---

## Next Steps

**Week 1 Action Items:**
1. ‚úÖ Run `scripts/transform_to_medcat_format.py`
2. ‚úÖ Validate CSV transformation quality
3. ‚úÖ Run `scripts/create_cdb_v2.py`
4. ‚úÖ Quick smoke test –∑ CustomCAT –Ω–∞ 2-3 synthetic texts

**Decision Point (End of Week 3):**
- Review Phase 1A validation results
- GO/NO-GO –Ω–∞ Phase 1B
- Plan production deployment timeline

**Long-term Vision:**
- –ú–æ–¥—É–ª—å–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–æ–∑–≤–æ–ª—è—î easy swap semantic backends
- –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö concepts –±–µ–∑ retraining
- Scale –¥–æ 70K+ concepts –±–µ–∑ architectural changes
- Foundation –¥–ª—è future Meta-CAT integration (context attributes)

---

## Appendix: Alternative Architectures Considered

### Alternative 1: Pure Semantic (Bio_ClinicalBERT Only)

**Rejected Rationale:** 
- Too slow –¥–ª—è large-scale processing
- Lower precision —á–µ—Ä–µ–∑ semantic ambiguity
- Requires expensive GPU infrastructure

### Alternative 2: scispaCy Instead of MedCAT

**Rejected Rationale:**
- Supervised approach requires labeled data (–Ω–µ–º–∞—î —É –Ω–∞—Å)
- Less flexible –¥–ª—è custom ontology integration
- Lower performance –Ω–∞ medical concepts (F1 ‚âà 0.65 vs MedCAT 0.80)

### Alternative 3: LLM-based (GPT-4, Claude)

**Rejected Rationale:**
- API costs prohibitive –¥–ª—è batch processing
- Latency unacceptable
- Hallucinations risk
- No offline capability

**Selected Architecture (Hybrid Dictionary + Semantic) Balance:**
- ‚úÖ Fast dictionary matching –¥–ª—è majority cases
- ‚úÖ Semantic fallback –¥–ª—è edge cases
- ‚úÖ Modular design –¥–ª—è future enhancements
- ‚úÖ Cost-effective infrastructure requirements

---

**Document Version:** 1.0
**Last Updated:** 2025-01-XX
**Status:** READY FOR IMPLEMENTATION